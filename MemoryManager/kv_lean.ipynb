{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5ad7d3",
   "metadata": {},
   "source": [
    "a decode loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a108ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a73d9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicalBlock:\n",
    "    def __init__(self, physical_block_id=None):\n",
    "        self.physical_block_id = physical_block_id  # None if on CPU\n",
    "        self.status = \"gpu\"  # or \"cpu\"\n",
    "        self.token_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "116d2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvictionPolicy:\n",
    "    def register(self, block): pass\n",
    "    def unregister(self, block): pass\n",
    "    def notify_use(self, block): pass\n",
    "    def evict(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b373169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class LRUEvictionPolicy(EvictionPolicy):\n",
    "    def __init__(self):\n",
    "        self.lru_queue = deque()\n",
    "\n",
    "    def register(self, block):\n",
    "        self.lru_queue.appendleft(block)\n",
    "\n",
    "    def unregister(self, block):\n",
    "        try:\n",
    "            self.lru_queue.remove(block)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    def notify_use(self, block):\n",
    "        try:\n",
    "            self.lru_queue.remove(block)\n",
    "            self.lru_queue.appendleft(block)\n",
    "        except ValueError:\n",
    "            pass  # block not tracked (e.g., on CPU)\n",
    "\n",
    "    def evict(self):\n",
    "        while self.lru_queue:\n",
    "            victim = self.lru_queue.pop()\n",
    "            if victim.status == \"gpu\":\n",
    "                return victim\n",
    "        raise RuntimeError(\"No GPU-resident blocks to evict.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f46e5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageTable:\n",
    "    def __init__(self, num_blocks, block_size, num_heads, head_dim, device=\"cuda\", eviction_policy=None):\n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = block_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.device = device\n",
    "\n",
    "        # Flat GPU memory pool\n",
    "        self.gpu_k = torch.zeros((num_blocks, block_size, num_heads, head_dim), device=device)\n",
    "        self.gpu_v = torch.zeros_like(self.gpu_k)\n",
    "\n",
    "        # CPU-stored evicted blocks: keyed by LogicalBlock object\n",
    "        self.cpu_k = {} # logical_block -> CPU tensor\n",
    "        self.cpu_v = {}\n",
    "\n",
    "        self.free_list = list(range(num_blocks))\n",
    "        self.eviction_policy = eviction_policy or LRUEvictionPolicy()\n",
    "\n",
    "    def allocate_block(self, logical_block):\n",
    "        \"\"\"Assign a physical block to a logical block.\"\"\"\n",
    "        if not self.free_list:\n",
    "            self._evict_one_block()\n",
    "\n",
    "        pid = self.free_list.pop()\n",
    "        logical_block.physical_block_id = pid\n",
    "        logical_block.status = \"gpu\"\n",
    "        self.eviction_policy.register(logical_block)\n",
    "        return pid\n",
    "\n",
    "    def _evict_one_block(self):\n",
    "        \"\"\"Evict a block found on the GPU based on eviction policy.\"\"\"\n",
    "        victim = self.eviction_policy.evict()\n",
    "        self.swap_to_cpu(victim)      \n",
    "\n",
    "    def swap_to_cpu(self, logical_block):\n",
    "        \"\"\"Move a block from GPU to CPU and free its physical block ID.\"\"\"\n",
    "        pid = logical_block.physical_block_id\n",
    "        if pid is None or logical_block.status != \"gpu\":\n",
    "            raise RuntimeError(\"Block is not on GPU or already swapped.\")\n",
    "\n",
    "        self.cpu_k[logical_block] = self.gpu_k[pid].clone().cpu()\n",
    "        # print(self.cpu_k[logical_block])\n",
    "        self.cpu_v[logical_block] = self.gpu_v[pid].clone().cpu()\n",
    "\n",
    "        logical_block.physical_block_id = None\n",
    "        logical_block.status = \"cpu\"\n",
    "        self.eviction_policy.unregister(logical_block)\n",
    "        self.free_list.append(pid)\n",
    "\n",
    "    def swap_to_gpu(self, logical_block):\n",
    "        \"\"\"Move a block from CPU to GPU, assigning a new physical block ID.\"\"\"\n",
    "        if logical_block not in self.cpu_k:\n",
    "            raise RuntimeError(\"Block not found in CPU cache.\")\n",
    "\n",
    "        if not self.free_list:\n",
    "            self._evict_one_block()\n",
    "\n",
    "        pid = self.free_list.pop()\n",
    "        self.gpu_k[pid] = self.cpu_k.pop(logical_block).to(self.device)\n",
    "        self.gpu_v[pid] = self.cpu_v.pop(logical_block).to(self.device)\n",
    "\n",
    "        logical_block.physical_block_id = pid\n",
    "        logical_block.status = \"gpu\"\n",
    "        self.eviction_policy.register(logical_block)\n",
    "\n",
    "    def resolve_block(self, logical_block):\n",
    "        \"\"\"Return the (K, V) tensors, swapping in if needed.\"\"\"\n",
    "        if logical_block.status == \"cpu\":\n",
    "            self.swap_to_gpu(logical_block)\n",
    "\n",
    "        self.eviction_policy.notify_use(logical_block)\n",
    "        pid = logical_block.physical_block_id\n",
    "        return self.gpu_k[pid], self.gpu_v[pid]\n",
    "\n",
    "    def free_block(self, logical_block):\n",
    "        \"\"\"Free both GPU and CPU copies of the block.\"\"\"\n",
    "        if logical_block.status == \"gpu\":\n",
    "            pid = logical_block.physical_block_id\n",
    "            self.gpu_k[pid].zero_()\n",
    "            self.gpu_v[pid].zero_()\n",
    "            self.free_list.append(pid)\n",
    "            self.eviction_policy.unregister(logical_block)\n",
    "        elif logical_block.status == \"cpu\":\n",
    "            del self.cpu_k[logical_block]\n",
    "            del self.cpu_v[logical_block]\n",
    "\n",
    "        logical_block.status = \"freed\"\n",
    "        logical_block.physical_block_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9534f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class KVCacheManager:\n",
    "    def __init__(self, page_table):\n",
    "        self.page_table = page_table\n",
    "        self.block_size = page_table.block_size\n",
    "        self.sequence_table = defaultdict(lambda: defaultdict(list))  # seq_id → layer_id → [LogicalBlock]\n",
    "\n",
    "    def _get_active_block(self, seq_id, layer_id):\n",
    "        blocks = self.sequence_table[seq_id][layer_id]\n",
    "        if blocks and blocks[-1].token_count < self.block_size:\n",
    "            return blocks[-1]\n",
    "\n",
    "        # Allocate a new logical block\n",
    "        new_block = LogicalBlock()\n",
    "        self.page_table.allocate_block(new_block)\n",
    "        self.sequence_table[seq_id][layer_id].append(new_block)\n",
    "        return new_block\n",
    "\n",
    "    def write_token(self, seq_id, layer_id, key_vec, value_vec):\n",
    "        block = self._get_active_block(seq_id, layer_id)\n",
    "        if block.token_count >= self.block_size:\n",
    "            raise RuntimeError(\"Attempted to write to full block.\")\n",
    "\n",
    "        # Ensure it's on GPU before writing\n",
    "        k_buf, v_buf = self.page_table.resolve_block(block) # will swap in if needed\n",
    "\n",
    "        idx = block.token_count\n",
    "        k_buf[idx] = key_vec\n",
    "        v_buf[idx] = value_vec\n",
    "        block.token_count += 1\n",
    "\n",
    "    def prefill(self, seq_id, layer_id, k_list, v_list):\n",
    "        for k, v in zip(k_list, v_list):\n",
    "            self.write_token(seq_id, layer_id, k, v)\n",
    "\n",
    "    def yield_k_blocks(self, seq_id, layer_id):\n",
    "        for block in self.sequence_table[seq_id][layer_id]:\n",
    "            k_buf, _ = self.page_table.resolve_block(block)\n",
    "            yield k_buf[:block.token_count]\n",
    "\n",
    "    def yield_v_blocks(self, seq_id, layer_id):\n",
    "        for block in self.sequence_table[seq_id][layer_id]:\n",
    "            _, v_buf = self.page_table.resolve_block(block)\n",
    "            yield v_buf[:block.token_count]\n",
    "\n",
    "    def yield_kv_blocks(self, seq_id, layer_id):\n",
    "        for block in self.sequence_table[seq_id][layer_id]:\n",
    "            k_buf, v_buf = self.page_table.resolve_block(block)\n",
    "            yield k_buf[:block.token_count], v_buf[:block.token_count]\n",
    "\n",
    "    def free(self, seq_id):\n",
    "        for layer_blocks in self.sequence_table[seq_id].values():\n",
    "            for block in layer_blocks:\n",
    "                self.page_table.free_block(block)\n",
    "        del self.sequence_table[seq_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e05f7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageTableWithSwap(PageTable):\n",
    "    def __init__(self, num_blocks, block_size, num_heads, head_dim, device=\"cuda\", k_cache=None, v_cache=None):\n",
    "        super().__init__(num_blocks, block_size, num_heads, head_dim, device=device)\n",
    "        self.k_cache = k_cache  # flat buffer: (1, H, max_tokens, D)\n",
    "        self.v_cache = v_cache\n",
    "        self.cpu_k = {}  # LogicalBlock → CPU tensor\n",
    "        self.cpu_v = {}\n",
    "\n",
    "    def swap_to_cpu(self, block):\n",
    "        pid = block.physical_block_id\n",
    "        if pid is None or block.status != \"gpu\":\n",
    "            raise RuntimeError(\"Block is not on GPU\")\n",
    "\n",
    "        bs = self.block_size\n",
    "        self.cpu_k[block] = self.k_cache[0, :, pid * bs : (pid + 1) * bs, :].clone().cpu()\n",
    "        self.cpu_v[block] = self.v_cache[0, :, pid * bs : (pid + 1) * bs, :].clone().cpu()\n",
    "\n",
    "        block.physical_block_id = None\n",
    "        block.status = \"cpu\"\n",
    "        self.eviction_policy.unregister(block)\n",
    "        self.free_list.append(pid)\n",
    "        print(\"sent\")\n",
    "\n",
    "    def swap_to_gpu(self, block):\n",
    "        if block.status != \"cpu\":\n",
    "            return\n",
    "\n",
    "        if not self.free_list:\n",
    "            victim = self.eviction_policy.evict()\n",
    "            self.swap_to_cpu(victim)\n",
    "\n",
    "        pid = self.free_list.pop()\n",
    "        bs = self.block_size\n",
    "\n",
    "        self.k_cache[0, :, pid * bs : (pid + 1) * bs, :] = self.cpu_k.pop(block).to(self.k_cache.device)\n",
    "        self.v_cache[0, :, pid * bs : (pid + 1) * bs, :] = self.cpu_v.pop(block).to(self.v_cache.device)\n",
    "\n",
    "        block.physical_block_id = pid\n",
    "        block.status = \"gpu\"\n",
    "        self.eviction_policy.register(block)\n",
    "        print(f\"retrieved to pid={pid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4cfdee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# === ATTENTION LAYER TO DROP-IN EDITED FOR DECODE===\n",
    "class SwappablePagedAttentionLayerBatched(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, head_dim, dtype, spa, max_pos=512):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.spa = spa\n",
    "        self.block_size = spa.page_size\n",
    "\n",
    "        self.wqkv = nn.Linear(hidden_dim, 3 * hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        self.wo = nn.Linear(hidden_dim, hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        # Inside __init__ of SwappablePagedAttentionLayerBatched\n",
    "        self.freqs_cis = precompute_freqs_cis(max_pos, head_dim, dtype=dtype).to(\"cuda\")\n",
    "\n",
    "\n",
    "    def forward(self, x, seq_ids, token_idxs):\n",
    "        B, S, _ = x.shape\n",
    "        is_decode = (x.shape[1] == 1)  # True if S == 1\n",
    "        x = x.to(dtype=self.wqkv.weight.dtype)\n",
    "        qkv = self.wqkv(x)\n",
    "        # q, k, v = qkv.split(self.hidden_dim, dim=-1)\n",
    "\n",
    "        # q = q.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        # k = k.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        # v = v.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        print(\"qkv.shape before reshape:\", qkv.shape)  # [B, S, 3 * hidden_dim]\n",
    "        print(\"⛏ RAW qkv.shape:\", qkv.shape)\n",
    "        qkv = qkv.view(B, S, 3, self.n_heads, self.head_dim)\n",
    "        print(\"🔧 After view →\", qkv.shape)\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4).contiguous()\n",
    "        print(\"🔄 After permute →\", qkv.shape)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        print(\"✅ Final k.shape:\", k.shape)\n",
    "\n",
    "        print(\"q.shape after reshape:\", q.shape)  # [B, H, S, D]\n",
    "        print(\"v.shape after reshape:\", v.shape)  # [B, H, S, D]\n",
    "        print(\"k.shape after reshape:\", k.shape)  # [B, H, S, D]\n",
    "\n",
    "        q, k = q.clone(), k.clone()\n",
    "        # for b, t in enumerate(token_idxs):\n",
    "        #     freqs = self.freqs_cis[t]\n",
    "        #     q[b:b+1] = apply_rotary_emb(q[b:b+1], freqs)\n",
    "        #     k[b:b+1] = apply_rotary_emb(k[b:b+1], freqs)\n",
    "        # for b, t in enumerate(token_idxs):\n",
    "        #     if t >= self.freqs_cis.shape[0]:\n",
    "        #         raise ValueError(f\"Token index {t} exceeds precomputed freqs_cis range\")\n",
    "        #     freqs = self.freqs_cis[t]\n",
    "        #     q[b:b+1] = apply_rotary_emb(q[b:b+1], freqs)\n",
    "        #     k[b:b+1] = apply_rotary_emb(k[b:b+1], freqs)\n",
    "        if S == 1:\n",
    "            # Decode step: rotate with single frequency (1 per token)\n",
    "            for b, t in enumerate(token_idxs):\n",
    "                freqs = self.freqs_cis[t]\n",
    "                # q[b] = apply_rotary_emb(q[b], freqs)  # q[b]: [H, 1, D]\n",
    "                # k[b] = apply_rotary_emb(k[b], freqs)  # k[b]: [H, 1, D]\n",
    "                q[b] = apply_rotary_emb(q[b].unsqueeze(0), freqs)[0]  # ✅ [1, H, S, D] → [H, S, D]\n",
    "                k[b] = apply_rotary_emb(k[b].unsqueeze(0), freqs)[0]\n",
    "\n",
    "                print(\"k[b].shape after applying rotary:\", k[b].shape)  # [B, H, S, D]\n",
    "\n",
    "        else:\n",
    "            # Prefill step: rotate with freqs for [0:S]\n",
    "            freqs = self.freqs_cis[:S].to(x.device)\n",
    "            q = apply_rotary_emb(q, freqs)\n",
    "            k = apply_rotary_emb(k, freqs)\n",
    "            print(\"k shape after else in S block: \", k.shape)\n",
    "        \n",
    "        for b in range(B):\n",
    "            assert k[b].ndim == 3 and k[b].shape[2] == self.head_dim, f\"BAD k[b] shape: {k[b].shape}\"\n",
    "            assert q[b].ndim == 3 and q[b].shape[2] == self.head_dim, f\"BAD q[b] shape: {q[b].shape}\"\n",
    "\n",
    "\n",
    "        for b, (sid, tidx) in enumerate(zip(seq_ids, token_idxs)):\n",
    "            print(\"assign_token → k[b].shape:\", k[b].shape)  # Should be [H, S, D]\n",
    "\n",
    "            self.spa.assign_token(sid, tidx, k[b], v[b])\n",
    "\n",
    "        block_mask = build_blockmask_batched(\n",
    "            page_tables=self.spa.page_table,\n",
    "            seq_ids=seq_ids,\n",
    "            token_idxs=token_idxs,\n",
    "            num_q=S,\n",
    "            total_kv=self.spa.max_flat_idx_written,\n",
    "            page_size=self.block_size,\n",
    "            k_cache=self.spa.k_cache\n",
    "        )\n",
    "\n",
    "        print(\"\\n🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\")\n",
    "        flat_kv_indices = block_mask.kv_indices.view(-1)\n",
    "        invalid = flat_kv_indices[(flat_kv_indices < 0) | (flat_kv_indices >= self.spa.k_cache.shape[2])]\n",
    "        print(\"🟥 Found invalid indices:\", invalid)\n",
    "        print(\"✅ Unique indices:\", torch.unique(flat_kv_indices))\n",
    "        print(\"✅ Max valid:\", self.spa.k_cache.shape[2] - 1)\n",
    "\n",
    "        out = self.spa.query(q, block_mask)\n",
    "        return self.wo(out.transpose(1, 2).contiguous().view(B, S, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcea29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PATCHED CAUSAL BLOCKMASK BUILDER ===\n",
    "def build_blockmask_batched(page_tables, seq_ids, token_idxs, num_q, total_kv, page_size, k_cache):\n",
    "    B, H = len(seq_ids), k_cache.shape[1]\n",
    "    device = k_cache.device\n",
    "\n",
    "    max_blocks = k_cache.shape[2] // page_size\n",
    "    kv_num_blocks = torch.zeros((B, H, 1), dtype=torch.int32, device=device)\n",
    "    kv_indices = torch.full((B, H, 1, max_blocks), -1, dtype=torch.int32, device=device)\n",
    "\n",
    "    for b, sid in enumerate(seq_ids):\n",
    "        raw_pids = page_tables[sid]\n",
    "        max_token_idx = token_idxs[b]\n",
    "        max_block_idx = max_token_idx // page_size\n",
    "\n",
    "        causal_pids = raw_pids[:max_block_idx + 1]\n",
    "        valid_pids = causal_pids[(causal_pids >= 0) & (causal_pids < max_blocks)]\n",
    "        num_valid = valid_pids.numel()\n",
    "\n",
    "        kv_num_blocks[b] = torch.full((H, 1), num_valid, dtype=torch.int32, device=device)\n",
    "        for h in range(H):\n",
    "            kv_indices[b, h, 0, :num_valid] = valid_pids[:max_blocks]\n",
    "            kv_indices[b, h, 0, num_valid:] = -1\n",
    "\n",
    "    assert (kv_indices < max_blocks).all() | (kv_indices == -1).all(), \"🟥 kv_indices out of range\"\n",
    "\n",
    "    return BlockMask.from_kv_blocks(\n",
    "        kv_num_blocks=kv_num_blocks,\n",
    "        kv_indices=kv_indices,\n",
    "        full_kv_num_blocks=None,\n",
    "        full_kv_indices=None,\n",
    "        BLOCK_SIZE=(num_q, page_size),\n",
    "        mask_mod=None,\n",
    "        seq_lengths=(num_q, total_kv),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be469a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## JUST FOR THE DECODE LOOP. DELETE FOR ABOVE TEST CASE RUN. JUST CHANGED ASSIGN TOKEN\n",
    "\n",
    "from torch.nn.attention.flex_attention import BlockMask\n",
    "class SwappablePagedAttention:\n",
    "    def __init__(self, kv_cache_manager, k_cache, v_cache, page_table_tensor, page_size, layer_id):\n",
    "        self.kv_cache_manager = kv_cache_manager\n",
    "        self.k_cache = k_cache\n",
    "        self.v_cache = v_cache\n",
    "        self.page_table = page_table_tensor\n",
    "        self.page_size = page_size\n",
    "        self.layer_id = layer_id\n",
    "        self.max_flat_idx_written = 0\n",
    "\n",
    "    def assign_token(self, seq_id, token_idx, key_vec, value_vec):\n",
    "        logical_block_id = token_idx // self.page_size\n",
    "        offset = token_idx % self.page_size\n",
    "\n",
    "        # ⬇️ Expand page_table if the logical block id exceeds current columns\n",
    "        if logical_block_id >= self.page_table.shape[1]:\n",
    "            pad = logical_block_id - self.page_table.shape[1] + 1\n",
    "            new_table = torch.full(\n",
    "                (self.page_table.shape[0], self.page_table.shape[1] + pad),\n",
    "                fill_value=-1,\n",
    "                dtype=self.page_table.dtype,\n",
    "                device=self.page_table.device\n",
    "            )\n",
    "            new_table[:, :self.page_table.shape[1]] = self.page_table\n",
    "            self.page_table = new_table\n",
    "\n",
    "        # Get or grow the block list for this sequence/layer\n",
    "        block_list = self.kv_cache_manager.sequence_table[seq_id][self.layer_id]\n",
    "        if len(block_list) <= logical_block_id:\n",
    "            for _ in range(logical_block_id - len(block_list) + 1):\n",
    "                new_block = LogicalBlock()\n",
    "                pid = self.kv_cache_manager.page_table.allocate_block(new_block)\n",
    "                self.kv_cache_manager.page_table.eviction_policy.register(new_block)\n",
    "                block_list.append(new_block)\n",
    "\n",
    "        block = block_list[logical_block_id]\n",
    "\n",
    "        # If it's not on GPU, swap it in\n",
    "        if block.status == \"cpu\":\n",
    "            self.kv_cache_manager.page_table.swap_to_gpu(block)\n",
    "\n",
    "        pid = block.physical_block_id\n",
    "        # Sanity check:\n",
    "        if block.physical_block_id is None:\n",
    "            raise RuntimeError(f\"Token {token_idx}: block still has no physical_block_id after swap.\")\n",
    "\n",
    "        flat_idx = pid * self.page_size + offset\n",
    "\n",
    "        # key_vec, value_vec: [H, S, D]\n",
    "        # token_idx: scalar index into S\n",
    "\n",
    "        # assert key_vec.ndim == 3, f\"Expected key_vec to be [H, S, D], got {key_vec.shape}\"\n",
    "        # assert value_vec.ndim == 3, f\"Expected value_vec to be [H, S, D], got {value_vec.shape}\"\n",
    "        # assert 0 <= token_idx < key_vec.shape[1], f\"token_idx {token_idx} out of bounds for seq_len={key_vec.shape[1]}\"\n",
    "\n",
    "        # self.k_cache[0, :, flat_idx, :] = key_vec[:, token_idx, :]  # [H, D]\n",
    "        # self.v_cache[0, :, flat_idx, :] = value_vec[:, token_idx, :]\n",
    "        print(\"[DEBUG] key_vec.shape:\", key_vec.shape)\n",
    "        print(\"[DEBUG] token_idx:\", token_idx)\n",
    "        # print(\"[DEBUG] key_vec.shape:\", key_vec.shape)\n",
    "        # print(\"[DEBUG] k_cache slice shape:\", self.k_cache[0, :, flat_idx, :].shape)\n",
    "        # Note: key_vec is [H, S, D], where usually S = 1 (decode) or up to T (prefill)\n",
    "\n",
    "        if key_vec.shape[1] == 1:\n",
    "            print(\"[DEBUG] decode mode → key_vec[:, 0, :].shape:\", key_vec[:, 0, :].shape)\n",
    "            self.k_cache[0, :, flat_idx, :] = key_vec[:, 0, :]  # decode\n",
    "            self.v_cache[0, :, flat_idx, :] = value_vec[:, 0, :]\n",
    "        else:\n",
    "            # token_idx is the global token index, but we only have local slice (0..S-1) — fix this:\n",
    "            # local_idx = token_idx % key_vec.shape[1]\n",
    "            local_idx = token_idx % key_vec.shape[1]\n",
    "            print(\"[DEBUG] key_vec[:, local_idx, :].shape:\", key_vec[:, local_idx, :].shape)\n",
    "            self.k_cache[0, :, flat_idx, :] = key_vec[:, local_idx, :]\n",
    "            self.v_cache[0, :, flat_idx, :] = value_vec[:, local_idx, :]\n",
    "            \n",
    "\n",
    "\n",
    "        print(\"hi\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        block.token_count += 1\n",
    "        if seq_id >= self.page_table.shape[0] or logical_block_id >= self.page_table.shape[1]:\n",
    "            print(f\"🚨 Resize needed: seq_id={seq_id}, logical_block_id={logical_block_id}, current shape={self.page_table.shape}\")\n",
    "\n",
    "        self.page_table[seq_id, logical_block_id] = pid\n",
    "\n",
    "        self.max_flat_idx_written = max(self.max_flat_idx_written, flat_idx + 1)\n",
    "\n",
    "        print(f\"[assign_token] token_idx={token_idx} | flat_idx={flat_idx}\")\n",
    "        if key_vec.shape[1] == 1:\n",
    "            print(f\"[assign_token] storing slice: {key_vec[:, 0, :].shape}\")\n",
    "        else:\n",
    "            local_idx = token_idx % key_vec.shape[1]\n",
    "            print(f\"[assign_token] storing slice: {key_vec[:, local_idx, :].shape}\")\n",
    "\n",
    "        # print(f\"[assign_token] storing slice: {key_vec[:, token_idx, :].shape}\")\n",
    "\n",
    "\n",
    "    def build_blockmask(self, num_query_tokens, total_tokens_written):\n",
    "        from torch.nn.attention.flex_attention import BlockMask\n",
    "\n",
    "        assert total_tokens_written <= self.k_cache.shape[2], \"KV length exceeds cache capacity\"\n",
    "        B, H = 1, self.k_cache.shape[1]\n",
    "        device = self.k_cache.device\n",
    "\n",
    "        logical_to_physical = self.page_table[0]  # single seq_id only\n",
    "        valid = logical_to_physical != -1\n",
    "        pids = logical_to_physical[valid]\n",
    "        total_blocks = pids.numel()\n",
    "\n",
    "        kv_indices = torch.full((B, H, 1, total_blocks), -1, dtype=torch.int32, device=device)\n",
    "        kv_num_blocks = torch.full((B, H, 1), total_blocks, dtype=torch.int32, device=device)\n",
    "\n",
    "        for h in range(H):\n",
    "            for i in range(total_blocks):\n",
    "                kv_indices[0, h, 0, i] = pids[i].item()\n",
    "        print(\">> max_flat_idx_written:\", self.max_flat_idx_written)\n",
    "        print(\">> kv_num_blocks.shape:\", kv_num_blocks.shape)\n",
    "        print(\">> kv_num_blocks:\", kv_num_blocks)\n",
    "        print(\">> kv_indices.shape:\", kv_indices.shape)\n",
    "        print(\">> kv_indices:\", kv_indices)\n",
    "\n",
    "        block_mask = BlockMask.from_kv_blocks(\n",
    "            kv_num_blocks=kv_num_blocks,\n",
    "            kv_indices=kv_indices,\n",
    "            full_kv_num_blocks=None,\n",
    "            full_kv_indices=None,\n",
    "            BLOCK_SIZE=(num_query_tokens, self.page_size),\n",
    "            mask_mod=None,\n",
    "            seq_lengths=(num_query_tokens, total_tokens_written)  # <- exact KV length\n",
    "        )\n",
    "\n",
    "        # Crop block mask to match token counts\n",
    "        return block_mask._adjust(num_query_tokens, total_tokens_written)\n",
    "\n",
    "    def query(self, q, block_mask):\n",
    "        from torch.nn.attention.flex_attention import flex_attention\n",
    "\n",
    "        # 🔁 Ensure required blocks are on GPU\n",
    "        for block_list in self.kv_cache_manager.sequence_table.values():\n",
    "            for block in block_list[self.layer_id]:\n",
    "                if block.status == \"cpu\":\n",
    "                    self.kv_cache_manager.page_table.swap_to_gpu(block)\n",
    "                    print(f\"⏪ Swapping in block with pid={block.physical_block_id}\")\n",
    "\n",
    "        print(\"Calling flex_attention with:\")\n",
    "        print(\"  q.shape:\", q.shape)\n",
    "        print(\"  k.shape:\", self.k_cache.shape)\n",
    "        print(\"  v.shape:\", self.v_cache.shape)\n",
    "\n",
    "\n",
    "        return flex_attention(\n",
    "            q, self.k_cache, self.v_cache,\n",
    "            block_mask=block_mask,\n",
    "            score_mod=None\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf447aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(seq_len, n_elem, base=10000, dtype=torch.float16):\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2).float() / n_elem))\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1).to(dtype=dtype, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea2492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2CompatibleAttention(nn.Module):\n",
    "    def __init__(self, swappable_layer, layer_id):\n",
    "        super().__init__()\n",
    "        self.attn_core = swappable_layer\n",
    "        self.layer_id = layer_id\n",
    "        self.token_idx = 0  # Tracks where we are in decoding\n",
    "\n",
    "    def forward(self, hidden_states, layer_past=None, attention_mask=None, **kwargs):\n",
    "        B, S, D = hidden_states.shape\n",
    "        seq_ids = list(range(B))\n",
    "        token_idxs = [self.token_idx] * B\n",
    "        self.token_idx += S\n",
    "\n",
    "        return (self.attn_core(hidden_states, seq_ids, token_idxs), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439a2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x, freqs_cis_t):\n",
    "    B, H, S, D = x.shape\n",
    "    assert D % 2 == 0, \"Head dim must be even\"\n",
    "\n",
    "    orig_dtype = x.dtype  # Save dtype to restore later\n",
    "    x = x.float().reshape(B, H, S, D // 2, 2)\n",
    "    freqs_cis_t = freqs_cis_t.unsqueeze(0).unsqueeze(0).unsqueeze(0)  # (1,1,1,D//2,2)\n",
    "\n",
    "    re = x[..., 0] * freqs_cis_t[..., 0] - x[..., 1] * freqs_cis_t[..., 1]\n",
    "    im = x[..., 1] * freqs_cis_t[..., 0] + x[..., 0] * freqs_cis_t[..., 1]\n",
    "\n",
    "    out = torch.stack((re, im), dim=-1).reshape(B, H, S, D)  # ✅ use reshape not flatten\n",
    "    return out.to(dtype=orig_dtype)  # restore original dtype safely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8401e32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] KV cache head_dim: 64\n",
      "qkv.shape before reshape: torch.Size([2, 4, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 4, 2304])\n",
      "🔧 After view → torch.Size([2, 4, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 4, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 4, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 4, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 4, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 4, 64])\n",
      "k shape after else in S block:  torch.Size([2, 12, 4, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 4, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 4, 64])\n",
      "[DEBUG] token_idx: 0\n",
      "[DEBUG] key_vec[:, local_idx, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=0 | flat_idx=31\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 4, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 4, 64])\n",
      "[DEBUG] token_idx: 0\n",
      "[DEBUG] key_vec[:, local_idx, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=0 | flat_idx=30\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1, 30, 31], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 4, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 4\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=4 | flat_idx=26\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 4\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=4 | flat_idx=22\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1, 22, 26, 30, 31], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 5\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=5 | flat_idx=21\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 5\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=5 | flat_idx=20\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1, 20, 21, 22, 26, 30, 31], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 6\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=6 | flat_idx=19\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 6\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=6 | flat_idx=18\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1, 18, 19, 20, 21, 22, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 7\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=7 | flat_idx=17\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 7\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=7 | flat_idx=16\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1, 16, 17, 18, 19, 20, 21, 22, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 8\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=8 | flat_idx=15\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 8\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=8 | flat_idx=14\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1, 14, 15, 16, 17, 18, 19, 20, 21, 22, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 9\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=9 | flat_idx=13\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 9\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=9 | flat_idx=12\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 26, 30, 31],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 10\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=10 | flat_idx=11\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 10\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=10 | flat_idx=10\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 26, 30, 31],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 11\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=11 | flat_idx=9\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 11\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=11 | flat_idx=8\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 26, 30,\n",
      "        31], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 12\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=12 | flat_idx=7\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 12\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=12 | flat_idx=6\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
      "        26, 30, 31], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 13\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=13 | flat_idx=5\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 13\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=13 | flat_idx=4\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
      "        21, 22, 26, 30, 31], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 14\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=14 | flat_idx=3\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 14\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=14 | flat_idx=2\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "        19, 20, 21, 22, 26, 30, 31], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 15\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=15 | flat_idx=1\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 15\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=15 | flat_idx=0\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 16\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=16 | flat_idx=31\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 16\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=16 | flat_idx=30\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "sent\n",
      "retrieved to pid=29\n",
      "⏪ Swapping in block with pid=29\n",
      "sent\n",
      "retrieved to pid=28\n",
      "⏪ Swapping in block with pid=28\n",
      "sent\n",
      "retrieved to pid=27\n",
      "⏪ Swapping in block with pid=27\n",
      "sent\n",
      "retrieved to pid=26\n",
      "⏪ Swapping in block with pid=26\n",
      "sent\n",
      "retrieved to pid=25\n",
      "⏪ Swapping in block with pid=25\n",
      "sent\n",
      "retrieved to pid=24\n",
      "⏪ Swapping in block with pid=24\n",
      "sent\n",
      "retrieved to pid=23\n",
      "⏪ Swapping in block with pid=23\n",
      "sent\n",
      "retrieved to pid=22\n",
      "⏪ Swapping in block with pid=22\n",
      "sent\n",
      "retrieved to pid=21\n",
      "⏪ Swapping in block with pid=21\n",
      "sent\n",
      "retrieved to pid=20\n",
      "⏪ Swapping in block with pid=20\n",
      "sent\n",
      "retrieved to pid=19\n",
      "⏪ Swapping in block with pid=19\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 17\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=17 | flat_idx=18\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 17\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=17 | flat_idx=17\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "sent\n",
      "retrieved to pid=16\n",
      "⏪ Swapping in block with pid=16\n",
      "sent\n",
      "retrieved to pid=15\n",
      "⏪ Swapping in block with pid=15\n",
      "sent\n",
      "retrieved to pid=14\n",
      "⏪ Swapping in block with pid=14\n",
      "sent\n",
      "retrieved to pid=13\n",
      "⏪ Swapping in block with pid=13\n",
      "sent\n",
      "retrieved to pid=12\n",
      "⏪ Swapping in block with pid=12\n",
      "sent\n",
      "retrieved to pid=11\n",
      "⏪ Swapping in block with pid=11\n",
      "sent\n",
      "retrieved to pid=10\n",
      "⏪ Swapping in block with pid=10\n",
      "sent\n",
      "retrieved to pid=9\n",
      "⏪ Swapping in block with pid=9\n",
      "sent\n",
      "retrieved to pid=8\n",
      "⏪ Swapping in block with pid=8\n",
      "sent\n",
      "retrieved to pid=7\n",
      "⏪ Swapping in block with pid=7\n",
      "sent\n",
      "retrieved to pid=6\n",
      "⏪ Swapping in block with pid=6\n",
      "sent\n",
      "retrieved to pid=5\n",
      "⏪ Swapping in block with pid=5\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 18\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=18 | flat_idx=4\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 18\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=18 | flat_idx=3\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=31\n",
      "⏪ Swapping in block with pid=31\n",
      "sent\n",
      "retrieved to pid=30\n",
      "⏪ Swapping in block with pid=30\n",
      "sent\n",
      "retrieved to pid=29\n",
      "⏪ Swapping in block with pid=29\n",
      "sent\n",
      "retrieved to pid=28\n",
      "⏪ Swapping in block with pid=28\n",
      "sent\n",
      "retrieved to pid=27\n",
      "⏪ Swapping in block with pid=27\n",
      "sent\n",
      "retrieved to pid=26\n",
      "⏪ Swapping in block with pid=26\n",
      "sent\n",
      "retrieved to pid=25\n",
      "⏪ Swapping in block with pid=25\n",
      "sent\n",
      "retrieved to pid=24\n",
      "⏪ Swapping in block with pid=24\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 19\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=19 | flat_idx=23\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 19\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=19 | flat_idx=22\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 23, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "sent\n",
      "retrieved to pid=21\n",
      "⏪ Swapping in block with pid=21\n",
      "sent\n",
      "retrieved to pid=20\n",
      "⏪ Swapping in block with pid=20\n",
      "sent\n",
      "retrieved to pid=19\n",
      "⏪ Swapping in block with pid=19\n",
      "sent\n",
      "retrieved to pid=18\n",
      "⏪ Swapping in block with pid=18\n",
      "sent\n",
      "retrieved to pid=17\n",
      "⏪ Swapping in block with pid=17\n",
      "sent\n",
      "retrieved to pid=16\n",
      "⏪ Swapping in block with pid=16\n",
      "sent\n",
      "retrieved to pid=15\n",
      "⏪ Swapping in block with pid=15\n",
      "sent\n",
      "retrieved to pid=14\n",
      "⏪ Swapping in block with pid=14\n",
      "sent\n",
      "retrieved to pid=13\n",
      "⏪ Swapping in block with pid=13\n",
      "sent\n",
      "retrieved to pid=12\n",
      "⏪ Swapping in block with pid=12\n",
      "sent\n",
      "retrieved to pid=11\n",
      "⏪ Swapping in block with pid=11\n",
      "sent\n",
      "retrieved to pid=10\n",
      "⏪ Swapping in block with pid=10\n",
      "sent\n",
      "retrieved to pid=9\n",
      "⏪ Swapping in block with pid=9\n",
      "sent\n",
      "retrieved to pid=8\n",
      "⏪ Swapping in block with pid=8\n",
      "sent\n",
      "retrieved to pid=7\n",
      "⏪ Swapping in block with pid=7\n",
      "sent\n",
      "retrieved to pid=6\n",
      "⏪ Swapping in block with pid=6\n",
      "sent\n",
      "retrieved to pid=5\n",
      "⏪ Swapping in block with pid=5\n",
      "sent\n",
      "retrieved to pid=4\n",
      "⏪ Swapping in block with pid=4\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 20\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=20 | flat_idx=0\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 20\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=20 | flat_idx=31\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 23, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "sent\n",
      "retrieved to pid=30\n",
      "⏪ Swapping in block with pid=30\n",
      "sent\n",
      "retrieved to pid=29\n",
      "⏪ Swapping in block with pid=29\n",
      "sent\n",
      "retrieved to pid=28\n",
      "⏪ Swapping in block with pid=28\n",
      "sent\n",
      "retrieved to pid=27\n",
      "⏪ Swapping in block with pid=27\n",
      "sent\n",
      "retrieved to pid=26\n",
      "⏪ Swapping in block with pid=26\n",
      "sent\n",
      "retrieved to pid=25\n",
      "⏪ Swapping in block with pid=25\n",
      "sent\n",
      "retrieved to pid=24\n",
      "⏪ Swapping in block with pid=24\n",
      "sent\n",
      "retrieved to pid=23\n",
      "⏪ Swapping in block with pid=23\n",
      "sent\n",
      "retrieved to pid=22\n",
      "⏪ Swapping in block with pid=22\n",
      "sent\n",
      "retrieved to pid=21\n",
      "⏪ Swapping in block with pid=21\n",
      "sent\n",
      "retrieved to pid=20\n",
      "⏪ Swapping in block with pid=20\n",
      "sent\n",
      "retrieved to pid=19\n",
      "⏪ Swapping in block with pid=19\n",
      "sent\n",
      "retrieved to pid=18\n",
      "⏪ Swapping in block with pid=18\n",
      "sent\n",
      "retrieved to pid=17\n",
      "⏪ Swapping in block with pid=17\n",
      "sent\n",
      "retrieved to pid=16\n",
      "⏪ Swapping in block with pid=16\n",
      "sent\n",
      "retrieved to pid=15\n",
      "⏪ Swapping in block with pid=15\n",
      "sent\n",
      "retrieved to pid=14\n",
      "⏪ Swapping in block with pid=14\n",
      "sent\n",
      "retrieved to pid=13\n",
      "⏪ Swapping in block with pid=13\n",
      "sent\n",
      "retrieved to pid=12\n",
      "⏪ Swapping in block with pid=12\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 21\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=21 | flat_idx=11\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 21\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=21 | flat_idx=10\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 23, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "sent\n",
      "retrieved to pid=9\n",
      "⏪ Swapping in block with pid=9\n",
      "sent\n",
      "retrieved to pid=8\n",
      "⏪ Swapping in block with pid=8\n",
      "sent\n",
      "retrieved to pid=7\n",
      "⏪ Swapping in block with pid=7\n",
      "sent\n",
      "retrieved to pid=6\n",
      "⏪ Swapping in block with pid=6\n",
      "sent\n",
      "retrieved to pid=5\n",
      "⏪ Swapping in block with pid=5\n",
      "sent\n",
      "retrieved to pid=4\n",
      "⏪ Swapping in block with pid=4\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=31\n",
      "⏪ Swapping in block with pid=31\n",
      "sent\n",
      "retrieved to pid=30\n",
      "⏪ Swapping in block with pid=30\n",
      "sent\n",
      "retrieved to pid=29\n",
      "⏪ Swapping in block with pid=29\n",
      "sent\n",
      "retrieved to pid=28\n",
      "⏪ Swapping in block with pid=28\n",
      "sent\n",
      "retrieved to pid=27\n",
      "⏪ Swapping in block with pid=27\n",
      "sent\n",
      "retrieved to pid=26\n",
      "⏪ Swapping in block with pid=26\n",
      "sent\n",
      "retrieved to pid=25\n",
      "⏪ Swapping in block with pid=25\n",
      "sent\n",
      "retrieved to pid=24\n",
      "⏪ Swapping in block with pid=24\n",
      "sent\n",
      "retrieved to pid=23\n",
      "⏪ Swapping in block with pid=23\n",
      "sent\n",
      "retrieved to pid=22\n",
      "⏪ Swapping in block with pid=22\n",
      "sent\n",
      "retrieved to pid=21\n",
      "⏪ Swapping in block with pid=21\n",
      "sent\n",
      "retrieved to pid=20\n",
      "⏪ Swapping in block with pid=20\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 22\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=22 | flat_idx=19\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 22\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=22 | flat_idx=18\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 23, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "sent\n",
      "retrieved to pid=17\n",
      "⏪ Swapping in block with pid=17\n",
      "sent\n",
      "retrieved to pid=16\n",
      "⏪ Swapping in block with pid=16\n",
      "sent\n",
      "retrieved to pid=15\n",
      "⏪ Swapping in block with pid=15\n",
      "sent\n",
      "retrieved to pid=14\n",
      "⏪ Swapping in block with pid=14\n",
      "sent\n",
      "retrieved to pid=13\n",
      "⏪ Swapping in block with pid=13\n",
      "sent\n",
      "retrieved to pid=12\n",
      "⏪ Swapping in block with pid=12\n",
      "sent\n",
      "retrieved to pid=11\n",
      "⏪ Swapping in block with pid=11\n",
      "sent\n",
      "retrieved to pid=10\n",
      "⏪ Swapping in block with pid=10\n",
      "sent\n",
      "retrieved to pid=9\n",
      "⏪ Swapping in block with pid=9\n",
      "sent\n",
      "retrieved to pid=8\n",
      "⏪ Swapping in block with pid=8\n",
      "sent\n",
      "retrieved to pid=7\n",
      "⏪ Swapping in block with pid=7\n",
      "sent\n",
      "retrieved to pid=6\n",
      "⏪ Swapping in block with pid=6\n",
      "sent\n",
      "retrieved to pid=5\n",
      "⏪ Swapping in block with pid=5\n",
      "sent\n",
      "retrieved to pid=4\n",
      "⏪ Swapping in block with pid=4\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=31\n",
      "⏪ Swapping in block with pid=31\n",
      "sent\n",
      "retrieved to pid=30\n",
      "⏪ Swapping in block with pid=30\n",
      "sent\n",
      "retrieved to pid=29\n",
      "⏪ Swapping in block with pid=29\n",
      "sent\n",
      "retrieved to pid=28\n",
      "⏪ Swapping in block with pid=28\n",
      "sent\n",
      "retrieved to pid=27\n",
      "⏪ Swapping in block with pid=27\n",
      "sent\n",
      "retrieved to pid=26\n",
      "⏪ Swapping in block with pid=26\n",
      "sent\n",
      "retrieved to pid=25\n",
      "⏪ Swapping in block with pid=25\n",
      "sent\n",
      "retrieved to pid=24\n",
      "⏪ Swapping in block with pid=24\n",
      "sent\n",
      "retrieved to pid=23\n",
      "⏪ Swapping in block with pid=23\n",
      "sent\n",
      "retrieved to pid=22\n",
      "⏪ Swapping in block with pid=22\n",
      "sent\n",
      "retrieved to pid=21\n",
      "⏪ Swapping in block with pid=21\n",
      "sent\n",
      "retrieved to pid=20\n",
      "⏪ Swapping in block with pid=20\n",
      "sent\n",
      "retrieved to pid=19\n",
      "⏪ Swapping in block with pid=19\n",
      "sent\n",
      "retrieved to pid=18\n",
      "⏪ Swapping in block with pid=18\n",
      "sent\n",
      "retrieved to pid=17\n",
      "⏪ Swapping in block with pid=17\n",
      "sent\n",
      "retrieved to pid=16\n",
      "⏪ Swapping in block with pid=16\n",
      "sent\n",
      "retrieved to pid=15\n",
      "⏪ Swapping in block with pid=15\n",
      "sent\n",
      "retrieved to pid=14\n",
      "⏪ Swapping in block with pid=14\n",
      "sent\n",
      "retrieved to pid=13\n",
      "⏪ Swapping in block with pid=13\n",
      "sent\n",
      "retrieved to pid=12\n",
      "⏪ Swapping in block with pid=12\n",
      "sent\n",
      "retrieved to pid=11\n",
      "⏪ Swapping in block with pid=11\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "qkv.shape before reshape: torch.Size([2, 1, 2304])\n",
      "⛏ RAW qkv.shape: torch.Size([2, 1, 2304])\n",
      "🔧 After view → torch.Size([2, 1, 3, 12, 64])\n",
      "🔄 After permute → torch.Size([2, 3, 12, 1, 64])\n",
      "✅ Final k.shape: torch.Size([2, 12, 1, 64])\n",
      "q.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "v.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k.shape after reshape: torch.Size([2, 12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "k[b].shape after applying rotary: torch.Size([12, 1, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 23\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=23 | flat_idx=10\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "assign_token → k[b].shape: torch.Size([12, 1, 64])\n",
      "sent\n",
      "[DEBUG] key_vec.shape: torch.Size([12, 1, 64])\n",
      "[DEBUG] token_idx: 23\n",
      "[DEBUG] decode mode → key_vec[:, 0, :].shape: torch.Size([12, 64])\n",
      "hi\n",
      "[assign_token] token_idx=23 | flat_idx=9\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 23, 26, 30, 31], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "sent\n",
      "retrieved to pid=8\n",
      "⏪ Swapping in block with pid=8\n",
      "sent\n",
      "retrieved to pid=7\n",
      "⏪ Swapping in block with pid=7\n",
      "sent\n",
      "retrieved to pid=6\n",
      "⏪ Swapping in block with pid=6\n",
      "sent\n",
      "retrieved to pid=5\n",
      "⏪ Swapping in block with pid=5\n",
      "sent\n",
      "retrieved to pid=4\n",
      "⏪ Swapping in block with pid=4\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=31\n",
      "⏪ Swapping in block with pid=31\n",
      "sent\n",
      "retrieved to pid=30\n",
      "⏪ Swapping in block with pid=30\n",
      "sent\n",
      "retrieved to pid=29\n",
      "⏪ Swapping in block with pid=29\n",
      "sent\n",
      "retrieved to pid=28\n",
      "⏪ Swapping in block with pid=28\n",
      "sent\n",
      "retrieved to pid=27\n",
      "⏪ Swapping in block with pid=27\n",
      "sent\n",
      "retrieved to pid=26\n",
      "⏪ Swapping in block with pid=26\n",
      "sent\n",
      "retrieved to pid=25\n",
      "⏪ Swapping in block with pid=25\n",
      "sent\n",
      "retrieved to pid=24\n",
      "⏪ Swapping in block with pid=24\n",
      "sent\n",
      "retrieved to pid=23\n",
      "⏪ Swapping in block with pid=23\n",
      "sent\n",
      "retrieved to pid=22\n",
      "⏪ Swapping in block with pid=22\n",
      "sent\n",
      "retrieved to pid=21\n",
      "⏪ Swapping in block with pid=21\n",
      "sent\n",
      "retrieved to pid=20\n",
      "⏪ Swapping in block with pid=20\n",
      "sent\n",
      "retrieved to pid=19\n",
      "⏪ Swapping in block with pid=19\n",
      "sent\n",
      "retrieved to pid=18\n",
      "⏪ Swapping in block with pid=18\n",
      "sent\n",
      "retrieved to pid=17\n",
      "⏪ Swapping in block with pid=17\n",
      "sent\n",
      "retrieved to pid=16\n",
      "⏪ Swapping in block with pid=16\n",
      "sent\n",
      "retrieved to pid=15\n",
      "⏪ Swapping in block with pid=15\n",
      "sent\n",
      "retrieved to pid=14\n",
      "⏪ Swapping in block with pid=14\n",
      "sent\n",
      "retrieved to pid=13\n",
      "⏪ Swapping in block with pid=13\n",
      "sent\n",
      "retrieved to pid=12\n",
      "⏪ Swapping in block with pid=12\n",
      "sent\n",
      "retrieved to pid=11\n",
      "⏪ Swapping in block with pid=11\n",
      "sent\n",
      "retrieved to pid=10\n",
      "⏪ Swapping in block with pid=10\n",
      "sent\n",
      "retrieved to pid=9\n",
      "⏪ Swapping in block with pid=9\n",
      "sent\n",
      "retrieved to pid=8\n",
      "⏪ Swapping in block with pid=8\n",
      "sent\n",
      "retrieved to pid=7\n",
      "⏪ Swapping in block with pid=7\n",
      "sent\n",
      "retrieved to pid=6\n",
      "⏪ Swapping in block with pid=6\n",
      "sent\n",
      "retrieved to pid=5\n",
      "⏪ Swapping in block with pid=5\n",
      "sent\n",
      "retrieved to pid=4\n",
      "⏪ Swapping in block with pid=4\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=31\n",
      "⏪ Swapping in block with pid=31\n",
      "sent\n",
      "retrieved to pid=30\n",
      "⏪ Swapping in block with pid=30\n",
      "sent\n",
      "retrieved to pid=29\n",
      "⏪ Swapping in block with pid=29\n",
      "sent\n",
      "retrieved to pid=28\n",
      "⏪ Swapping in block with pid=28\n",
      "sent\n",
      "retrieved to pid=27\n",
      "⏪ Swapping in block with pid=27\n",
      "sent\n",
      "retrieved to pid=26\n",
      "⏪ Swapping in block with pid=26\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 12, 1, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "\n",
      "=== Outputs ===\n",
      "[0] The sky isThe first, and the same, and the same\n",
      "[1] Once upon a time, and the same, and the same, and\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# --- Setup ---\n",
    "device = \"cuda\"\n",
    "batch_size = 2\n",
    "max_new_tokens = 10\n",
    "D = 64\n",
    "H = 12\n",
    "hidden_dim = H * D\n",
    "num_blocks = 32\n",
    "page_size = 1\n",
    "dtype = torch.float16\n",
    "\n",
    "k_cache = torch.zeros((1, H, num_blocks * page_size, D), device=device, dtype=torch.float16)\n",
    "v_cache = torch.zeros_like(k_cache)\n",
    "print(\"[CHECK] KV cache head_dim:\", k_cache.shape[-1])  # Should be 64\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Needed for padding\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").eval().to(device)\n",
    "model.config.use_cache = False  # Important for our SPA patch\n",
    "\n",
    "pt = PageTableWithSwap(num_blocks, page_size, H, D, device=device, k_cache=k_cache, v_cache=v_cache)\n",
    "manager = KVCacheManager(page_table=pt)\n",
    "\n",
    "# === SPA INSTANCE ===\n",
    "spa = SwappablePagedAttention(\n",
    "    kv_cache_manager=manager,\n",
    "    k_cache=k_cache,\n",
    "    v_cache=v_cache,\n",
    "    page_table_tensor=torch.full((batch_size, 1024), -1, dtype=torch.int32, device=device),  # GPT-2 max length\n",
    "    page_size=page_size,\n",
    "    layer_id=0\n",
    ")\n",
    "\n",
    "# === PATCHED LAYER ===\n",
    "patched_layer = SwappablePagedAttentionLayerBatched(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=H,\n",
    "    head_dim=D,\n",
    "    dtype=dtype,\n",
    "    spa=spa\n",
    ")\n",
    "\n",
    "# === PATCH FIRST LAYER WITH CUSTOM ATTENTION ===\n",
    "model.transformer.h[0].attn = GPT2CompatibleAttention(patched_layer, layer_id=0)\n",
    "\n",
    "\n",
    "# --- Input Prompts ---\n",
    "prompts = [\n",
    "    \"The sky is\",\n",
    "    \"Once upon a time\"\n",
    "]\n",
    "input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)\n",
    "\n",
    "# Track lengths of each prompt (to know where decode starts)\n",
    "prompt_lengths = attention_mask.sum(dim=1)\n",
    "\n",
    "# --- Prefill (token_idx = 0 → prompt_lengths-1) ---\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "    generated = [ids.clone() for ids in input_ids]  # List of token sequences\n",
    "\n",
    "# --- Decode Loop ---\n",
    "for step in range(max_new_tokens):\n",
    "    # last_tokens = torch.stack([g[prompt_lengths[i] + step - 1] for i, g in enumerate(generated)])\n",
    "    # last_tokens = last_tokens.unsqueeze(1).to(device)  # (B, 1)\n",
    "    # Only pass the last generated token for each sequence\n",
    "    last_tokens = torch.stack([g[-1] for g in generated])  # (B,)\n",
    "    last_tokens = last_tokens.unsqueeze(1)                # (B, 1)\n",
    "\n",
    "    # Now run the model\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=last_tokens)                # Correct shape: (B, 1)\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=last_tokens)  # Forward 1 token per sequence\n",
    "\n",
    "        next_logits = out.logits[:, -1, :]  # (B, vocab_size)\n",
    "        next_tokens = torch.argmax(next_logits, dim=-1)  # Greedy decode (B,)\n",
    "\n",
    "        # Append to each sequence\n",
    "        for i in range(batch_size):\n",
    "            generated[i] = torch.cat([generated[i], next_tokens[i:i+1]], dim=0)\n",
    "\n",
    "# --- Decode output ---\n",
    "decoded = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n",
    "print(\"\\n=== Outputs ===\")\n",
    "for i, text in enumerate(decoded):\n",
    "    print(f\"[{i}] {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effai_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
