{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8ddba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1016260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GPT-2 and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4a2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of your life is in your hands.\n",
      "\n",
      "I'm not saying that you should be ashamed of your past, but I'm saying that\n"
     ]
    }
   ],
   "source": [
    "# Sample input\n",
    "prompt = \"The future of your life is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# Run baseline inference\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=30)\n",
    "    print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b57a5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of layers: 12\n",
      "Key shape (layer 0): torch.Size([1, 12, 6, 64])\n",
      "Value shape (layer 0): torch.Size([1, 12, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, use_cache=True)\n",
    "    past_key_values = outputs.past_key_values\n",
    "\n",
    "# Inspect shape of past key/value for layer 0\n",
    "print(f\"# of layers: {len(past_key_values)}\")\n",
    "print(f\"Key shape (layer 0): {past_key_values[0][0].shape}\")  # [B, n_heads, seq_len, head_dim]\n",
    "print(f\"Value shape (layer 0): {past_key_values[0][1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14717982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 12, Heads: 12, Hidden: 768, Head dim: 64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "config = model.config\n",
    "print(f\"Layers: {config.n_layer}, Heads: {config.n_head}, Hidden: {config.n_embd}, Head dim: {config.n_embd // config.n_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb2eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicalBlock:\n",
    "    def __init__(self, physical_block_id=None):\n",
    "        self.physical_block_id = physical_block_id  # None if on CPU\n",
    "        self.status = \"gpu\"  # or \"cpu\"\n",
    "        self.token_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d4bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvictionPolicy:\n",
    "    def register(self, block): pass\n",
    "    def unregister(self, block): pass\n",
    "    def notify_use(self, block): pass\n",
    "    def evict(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6902c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class LRUEvictionPolicy(EvictionPolicy):\n",
    "    def __init__(self):\n",
    "        self.lru_queue = deque()\n",
    "\n",
    "    def register(self, block):\n",
    "        self.lru_queue.appendleft(block)\n",
    "\n",
    "    def unregister(self, block):\n",
    "        try:\n",
    "            self.lru_queue.remove(block)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    def notify_use(self, block):\n",
    "        try:\n",
    "            self.lru_queue.remove(block)\n",
    "            self.lru_queue.appendleft(block)\n",
    "        except ValueError:\n",
    "            pass  # block not tracked (e.g., on CPU)\n",
    "\n",
    "    def evict(self):\n",
    "        while self.lru_queue:\n",
    "            victim = self.lru_queue.pop()\n",
    "            if victim.status == \"gpu\":\n",
    "                return victim\n",
    "        raise RuntimeError(\"No GPU-resident blocks to evict.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179901cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageTable:\n",
    "    def __init__(self, num_blocks, block_size, num_heads, head_dim, device=\"cuda\", eviction_policy=None):\n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = block_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.device = device\n",
    "\n",
    "        # Flat GPU memory pool\n",
    "        self.gpu_k = torch.zeros((num_blocks, block_size, num_heads, head_dim), device=device)\n",
    "        self.gpu_v = torch.zeros_like(self.gpu_k)\n",
    "\n",
    "        # CPU-stored evicted blocks: keyed by LogicalBlock object\n",
    "        self.cpu_k = {} # logical_block -> CPU tensor\n",
    "        self.cpu_v = {}\n",
    "\n",
    "        self.free_list = list(range(num_blocks))\n",
    "        self.eviction_policy = eviction_policy or LRUEvictionPolicy()\n",
    "\n",
    "    def allocate_block(self, logical_block):\n",
    "        \"\"\"Assign a physical block to a logical block.\"\"\"\n",
    "        if not self.free_list:\n",
    "            self._evict_one_block()\n",
    "\n",
    "        pid = self.free_list.pop()\n",
    "        logical_block.physical_block_id = pid\n",
    "        logical_block.status = \"gpu\"\n",
    "        self.eviction_policy.register(logical_block)\n",
    "        return pid\n",
    "\n",
    "    def _evict_one_block(self):\n",
    "        \"\"\"Evict a block found on the GPU based on eviction policy.\"\"\"\n",
    "        victim = self.eviction_policy.evict()\n",
    "        self.swap_to_cpu(victim)      \n",
    "\n",
    "    def swap_to_cpu(self, logical_block):\n",
    "        \"\"\"Move a block from GPU to CPU and free its physical block ID.\"\"\"\n",
    "        pid = logical_block.physical_block_id\n",
    "        if pid is None or logical_block.status != \"gpu\":\n",
    "            raise RuntimeError(\"Block is not on GPU or already swapped.\")\n",
    "\n",
    "        self.cpu_k[logical_block] = self.gpu_k[pid].clone().cpu()\n",
    "        # print(self.cpu_k[logical_block])\n",
    "        self.cpu_v[logical_block] = self.gpu_v[pid].clone().cpu()\n",
    "\n",
    "        logical_block.physical_block_id = None\n",
    "        logical_block.status = \"cpu\"\n",
    "        self.eviction_policy.unregister(logical_block)\n",
    "        self.free_list.append(pid)\n",
    "\n",
    "    def swap_to_gpu(self, logical_block):\n",
    "        \"\"\"Move a block from CPU to GPU, assigning a new physical block ID.\"\"\"\n",
    "        if logical_block not in self.cpu_k:\n",
    "            raise RuntimeError(\"Block not found in CPU cache.\")\n",
    "\n",
    "        if not self.free_list:\n",
    "            self._evict_one_block()\n",
    "\n",
    "        pid = self.free_list.pop()\n",
    "        self.gpu_k[pid] = self.cpu_k.pop(logical_block).to(self.device)\n",
    "        self.gpu_v[pid] = self.cpu_v.pop(logical_block).to(self.device)\n",
    "\n",
    "        logical_block.physical_block_id = pid\n",
    "        logical_block.status = \"gpu\"\n",
    "        self.eviction_policy.register(logical_block)\n",
    "\n",
    "    def resolve_block(self, logical_block):\n",
    "        \"\"\"Return the (K, V) tensors, swapping in if needed.\"\"\"\n",
    "        if logical_block.status == \"cpu\":\n",
    "            self.swap_to_gpu(logical_block)\n",
    "\n",
    "        self.eviction_policy.notify_use(logical_block)\n",
    "        pid = logical_block.physical_block_id\n",
    "        return self.gpu_k[pid], self.gpu_v[pid]\n",
    "\n",
    "    def free_block(self, logical_block):\n",
    "        \"\"\"Free both GPU and CPU copies of the block.\"\"\"\n",
    "        if logical_block.status == \"gpu\":\n",
    "            pid = logical_block.physical_block_id\n",
    "            self.gpu_k[pid].zero_()\n",
    "            self.gpu_v[pid].zero_()\n",
    "            self.free_list.append(pid)\n",
    "            self.eviction_policy.unregister(logical_block)\n",
    "        elif logical_block.status == \"cpu\":\n",
    "            del self.cpu_k[logical_block]\n",
    "            del self.cpu_v[logical_block]\n",
    "\n",
    "        logical_block.status = \"freed\"\n",
    "        logical_block.physical_block_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad851df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logical block and allocate it\n",
    "block = LogicalBlock()\n",
    "page_table = PageTable(num_blocks=4, block_size=2, num_heads=2, head_dim=4, device=\"cuda\")\n",
    "pid = page_table.allocate_block(block)\n",
    "\n",
    "# Set some known values\n",
    "page_table.gpu_k[pid][0][0] = torch.tensor([1., 1., 1., 1.])\n",
    "page_table.gpu_v[pid][1][1] = torch.tensor([2., 2., 2., 2.])\n",
    "\n",
    "# Swap to CPU and back\n",
    "page_table.swap_to_cpu(block)\n",
    "page_table.swap_to_gpu(block)\n",
    "\n",
    "# Resolve and print\n",
    "restored_k, restored_v = page_table.resolve_block(block)\n",
    "print(\"K_block[0,0]:\", restored_k[0, 0])  # should be [1, 1, 1, 1]\n",
    "print(\"V_block[1,1]:\", restored_v[1, 1])  # should be [2, 2, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68af353",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = 2\n",
    "page_table = PageTable(num_blocks=num_blocks, block_size=1, num_heads=1, head_dim=2, device=\"cpu\")\n",
    "\n",
    "# Allocate 3 blocks to force eviction\n",
    "blocks = [LogicalBlock() for _ in range(3)]\n",
    "\n",
    "for i, blk in enumerate(blocks):\n",
    "    page_table.allocate_block(blk)\n",
    "    # print(blk)\n",
    "    k, v = page_table.resolve_block(blk)\n",
    "    k[0, 0] = torch.tensor([i + 1.0, i + 1.0])    # Set K\n",
    "    v[0, 0] = torch.tensor([(i + 1) * 10.0] * 2)  # Set V\n",
    "\n",
    "# for i, blk in enumerate(blocks):\n",
    "#     print(blk.status)\n",
    "\n",
    "# print(page_table.cpu_k)\n",
    "\n",
    "# Read them back to verify swap-in works\n",
    "for i, blk in enumerate(blocks):\n",
    "    k, v = page_table.resolve_block(blk)\n",
    "    print(f\"Block {i} K:\", k[0, 0].tolist(), \" V:\", v[0, 0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38d6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class KVCacheManager:\n",
    "    def __init__(self, page_table):\n",
    "        self.page_table = page_table\n",
    "        self.block_size = page_table.block_size\n",
    "        self.sequence_table = defaultdict(lambda: defaultdict(list))  # seq_id → layer_id → [LogicalBlock]\n",
    "\n",
    "    def _get_active_block(self, seq_id, layer_id):\n",
    "        blocks = self.sequence_table[seq_id][layer_id]\n",
    "        if blocks and blocks[-1].token_count < self.block_size:\n",
    "            return blocks[-1]\n",
    "\n",
    "        # Allocate a new logical block\n",
    "        new_block = LogicalBlock()\n",
    "        self.page_table.allocate_block(new_block)\n",
    "        self.sequence_table[seq_id][layer_id].append(new_block)\n",
    "        return new_block\n",
    "\n",
    "    def write_token(self, seq_id, layer_id, key_vec, value_vec):\n",
    "        block = self._get_active_block(seq_id, layer_id)\n",
    "        if block.token_count >= self.block_size:\n",
    "            raise RuntimeError(\"Attempted to write to full block.\")\n",
    "\n",
    "        # Ensure it's on GPU before writing\n",
    "        k_buf, v_buf = self.page_table.resolve_block(block) # will swap in if needed\n",
    "\n",
    "        idx = block.token_count\n",
    "        k_buf[idx] = key_vec\n",
    "        v_buf[idx] = value_vec\n",
    "        block.token_count += 1\n",
    "\n",
    "    def prefill(self, seq_id, layer_id, k_list, v_list):\n",
    "        for k, v in zip(k_list, v_list):\n",
    "            self.write_token(seq_id, layer_id, k, v)\n",
    "\n",
    "    def yield_k_blocks(self, seq_id, layer_id):\n",
    "        for block in self.sequence_table[seq_id][layer_id]:\n",
    "            k_buf, _ = self.page_table.resolve_block(block)\n",
    "            yield k_buf[:block.token_count]\n",
    "\n",
    "    def yield_v_blocks(self, seq_id, layer_id):\n",
    "        for block in self.sequence_table[seq_id][layer_id]:\n",
    "            _, v_buf = self.page_table.resolve_block(block)\n",
    "            yield v_buf[:block.token_count]\n",
    "\n",
    "    def yield_kv_blocks(self, seq_id, layer_id):\n",
    "        for block in self.sequence_table[seq_id][layer_id]:\n",
    "            k_buf, v_buf = self.page_table.resolve_block(block)\n",
    "            yield k_buf[:block.token_count], v_buf[:block.token_count]\n",
    "\n",
    "    def free(self, seq_id):\n",
    "        for layer_blocks in self.sequence_table[seq_id].values():\n",
    "            for block in layer_blocks:\n",
    "                self.page_table.free_block(block)\n",
    "        del self.sequence_table[seq_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "num_blocks = 2\n",
    "block_size = 2\n",
    "num_heads = 2\n",
    "head_dim = 4\n",
    "device = \"cuda\"\n",
    "\n",
    "page_table = PageTable(num_blocks, block_size, num_heads, head_dim, device)\n",
    "manager = KVCacheManager(page_table)\n",
    "\n",
    "# Simulate a prompt for 1 sequence, 1 layer\n",
    "seq_id = 0\n",
    "layer_id = 0\n",
    "\n",
    "# Write 4 tokens (fills 2 blocks)\n",
    "for i in range(4):\n",
    "    k = torch.full((num_heads, head_dim), i + 1.0, device=device)\n",
    "    v = torch.full((num_heads, head_dim), (i + 1) * 10.0, device=device)\n",
    "    manager.write_token(seq_id, layer_id, k, v)\n",
    "\n",
    "# Print all resolved KV blocks\n",
    "print(\"\\n--- Resolved Blocks ---\")\n",
    "for i, (k, v) in enumerate(manager.yield_kv_blocks(seq_id, layer_id)):\n",
    "    print(f\"Block {i}:\")\n",
    "    print(\"  K:\\n\", k)\n",
    "    print(\"  V:\\n\", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd90ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch.nn.attention.flex_attention import (\n",
    "    _identity,\n",
    "    _mask_mod_signature,\n",
    "    _score_mod_signature,\n",
    "    BlockMask,\n",
    "    noop_mask,\n",
    ")\n",
    "\n",
    "\n",
    "def _cdiv(x: Union[int, float, torch.Tensor], multiple: Union[int, float, torch.Tensor]):\n",
    "    return (x + multiple - 1) // multiple\n",
    "\n",
    "\n",
    "class PagedAttention:\n",
    "    \"\"\"\n",
    "    PagedAttention supports flex attention inference with a large batch size.\n",
    "    With PagedAttention, a batch of key/value tensors with varying kv length\n",
    "    is splitted into tensor blocks of fixed length and cached in a compact way.\n",
    "    Thus we can avoid redundant memory consumption due to varying kv length and\n",
    "    support a larger batch size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_pages: int,\n",
    "        page_size: int,\n",
    "        max_batch_size: int,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        # number of pages\n",
    "        self.n_pages = n_pages\n",
    "\n",
    "        # number of tokens per page\n",
    "        self.page_size = page_size\n",
    "\n",
    "        # page table: [batch, logical_block_idx] -> physical_page_idx\n",
    "        self.page_table = -torch.ones(\n",
    "            (max_batch_size, self.n_pages), dtype=torch.int64, device=device\n",
    "        )\n",
    "\n",
    "        # capacity: batch_idx -> allocated sequence length\n",
    "        self.capacity = torch.zeros(max_batch_size, dtype=torch.int64, device=device)\n",
    "\n",
    "        # index of empty pages that is available for allocation\n",
    "        self.empty_pages = list(range(n_pages - 1, -1, -1))\n",
    "\n",
    "        # mapping from physical page index to logical page index\n",
    "        self.physical_to_logical = -torch.ones(\n",
    "            (max_batch_size, n_pages), dtype=torch.int64, device=device\n",
    "        )\n",
    "\n",
    "    def reserve(self, batch_idx: torch.Tensor, seq_len: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Requests the capacity of a given batch to be at least enough to\n",
    "        hold `seq_len` elements.\n",
    "\n",
    "        Args:\n",
    "            batch_idx (Tensor): batch index to be reserved; shape :math:`(1)`.\n",
    "            seq_len (Tensor): minimum capacity for the given batch; shape :math:`(1)`.\n",
    "        \"\"\"\n",
    "\n",
    "        if seq_len <= self.capacity[batch_idx]:\n",
    "            return\n",
    "\n",
    "        num_pages_to_allocate = _cdiv(seq_len - self.capacity[batch_idx], self.page_size)\n",
    "\n",
    "        assert len(self.empty_pages) >= num_pages_to_allocate, (\n",
    "            f\"requested {num_pages_to_allocate.item()} pages \"\n",
    "            f\"but there are only {len(self.empty_pages)} empty pages\"\n",
    "        )\n",
    "\n",
    "        start_page_idx = self.capacity[batch_idx] // self.page_size\n",
    "        end_page_idx = start_page_idx + num_pages_to_allocate\n",
    "\n",
    "        # find empty physical pages\n",
    "        allocated_pages = torch.tensor(\n",
    "            self.empty_pages[-num_pages_to_allocate:],\n",
    "            device=num_pages_to_allocate.device,\n",
    "        )\n",
    "        self.empty_pages = self.empty_pages[:-num_pages_to_allocate]\n",
    "\n",
    "        # update page table\n",
    "        self.page_table[\n",
    "            batch_idx,\n",
    "            start_page_idx:end_page_idx,\n",
    "        ] = allocated_pages\n",
    "\n",
    "        # update metadata\n",
    "        self.physical_to_logical[batch_idx, allocated_pages] = torch.arange(\n",
    "            start_page_idx.item(),\n",
    "            end_page_idx.item(),\n",
    "            device=num_pages_to_allocate.device,\n",
    "        )\n",
    "        self.capacity[batch_idx] += num_pages_to_allocate * self.page_size\n",
    "\n",
    "    def erase(self, batch_idx: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Removes a single batch from paged attention.\n",
    "\n",
    "        Args:\n",
    "            batch_idx (Tensor): batch index to be removed; shape :math:`(1)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # find allocated pages\n",
    "        allocated_page_idx = self.page_table[batch_idx] != -1\n",
    "        allocated_pages = self.page_table[batch_idx][allocated_page_idx]\n",
    "\n",
    "        # clean metadata\n",
    "        self.capacity[batch_idx] = 0\n",
    "        self.empty_pages += allocated_pages.tolist()\n",
    "        self.physical_to_logical[batch_idx][:, allocated_pages] = -1\n",
    "        self.page_table[batch_idx] = -1\n",
    "\n",
    "    def assign(\n",
    "        self,\n",
    "        batch_idx: torch.Tensor,\n",
    "        input_pos: torch.Tensor,\n",
    "        k_val: torch.Tensor,\n",
    "        v_val: torch.Tensor,\n",
    "        k_cache: torch.Tensor,\n",
    "        v_cache: torch.Tensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Assigns new contents `val` to the storage `cache` at the location\n",
    "        `batch_idx` and `input_pos`.\n",
    "\n",
    "        Args:\n",
    "            batch_idx (Tensor): batch index; shape :math:`(B)`.\n",
    "            input_pos (Tensor): input positions to be assigned for the given batch; shape :math:`(B, S)`.\n",
    "            val (Tensor): value to be assigned; shape :math:`(B, H, S, D)`\n",
    "            cache (Tensor): the cache to store the values; shape:`(1, H, MAX_S, D)`\n",
    "        \"\"\"\n",
    "        if k_val.requires_grad:\n",
    "            raise RuntimeError(\"val must not require gradient\")\n",
    "\n",
    "        B, H, S, K_D = k_val.shape\n",
    "        V_D = v_val.shape[3]\n",
    "        if B != batch_idx.shape[0]:\n",
    "            raise RuntimeError(\n",
    "                f\"Expect val and batch_idx have the same batch size \"\n",
    "                f\"but got B={B} and B={batch_idx.shape[0]}.\"\n",
    "            )\n",
    "        if H != k_cache.shape[1]:\n",
    "            raise RuntimeError(\n",
    "                f\"Expect val and cache has the same number of heads \"\n",
    "                f\"but got H={H} and H={k_cache.shape[1]}.\"\n",
    "            )\n",
    "        if S != input_pos.shape[1]:\n",
    "            raise RuntimeError(\n",
    "                f\"Expect val and input_pos has the same length \"\n",
    "                f\"but got S={S} and S={input_pos.shape[0]}.\"\n",
    "            )\n",
    "        if K_D != k_cache.shape[3]:\n",
    "            raise RuntimeError(\n",
    "                f\"Expect k_val and k_cache has the same hidden dim \"\n",
    "                f\"but got D={K_D} and D={k_cache.shape[3]}.\"\n",
    "            )\n",
    "        if V_D != v_cache.shape[3]:\n",
    "            raise RuntimeError(\n",
    "                f\"Expect v_val and v_cache has the same hidden dim \"\n",
    "                f\"but got D={V_D} and D={v_cache.shape[3]}.\"\n",
    "            )\n",
    "\n",
    "        # find address\n",
    "        logical_block_idx = input_pos // self.page_size  # [B, S]\n",
    "        logical_block_offset = input_pos % self.page_size  # [B, S]\n",
    "        physical_block_idx = torch.gather(\n",
    "            self.page_table[batch_idx], 1, logical_block_idx.to(torch.int64)\n",
    "        ).to(torch.int32)  # [B, S]\n",
    "\n",
    "        addr = (physical_block_idx * self.page_size + logical_block_offset).view(-1)  # [B*S]\n",
    "\n",
    "        k_val = k_val.permute(1, 0, 2, 3).contiguous().view(1, H, B * S, K_D)\n",
    "        v_val = v_val.permute(1, 0, 2, 3).contiguous().view(1, H, B * S, V_D)\n",
    "\n",
    "        k_cache[:, :, addr, :] = k_val\n",
    "        v_cache[:, :, addr, :] = v_val\n",
    "\n",
    "    def convert_logical_block_mask(\n",
    "        self,\n",
    "        block_mask: BlockMask,\n",
    "        batch_idx: Optional[torch.Tensor] = None,\n",
    "    ) -> BlockMask:\n",
    "        \"\"\"\n",
    "        Converts a logical block mask by mapping its logical kv indices to the corresponding\n",
    "        physical kv indices.\n",
    "\n",
    "        Args:\n",
    "            block_mask (BlockMask): logical block mask;\n",
    "                kv_indices shape :math:`(B, H, ROWS, MAX_BLOCKS_IN_COL)`.\n",
    "            batch_idx (Tensor): batch index corresponding to the block_mask\n",
    "                batch dimension. This provides flexibility to convert a\n",
    "                block mask with smaller batch size than the page table;\n",
    "                shape :math:`(B)`.\n",
    "        \"\"\"\n",
    "        B, H, ROWS, MAX_BLOCKS_IN_COL = block_mask.kv_indices.shape\n",
    "\n",
    "        if block_mask.BLOCK_SIZE[1] != self.page_size:\n",
    "            raise RuntimeError(\n",
    "                f\"Expect block_mask has the same column block size as page_size\"\n",
    "                f\"but got size={block_mask.BLOCK_SIZE[1]} and size={self.page_size}\"\n",
    "            )\n",
    "\n",
    "        # Increase the num columns of converted block mask from logical block mask's\n",
    "        # num columns to n_pages, since a) the converted block mask\n",
    "        # may have larger indices values; and b) `_ordered_to_dense` realizes\n",
    "        # a dense tensor with these converted indices. There would be an IndexError\n",
    "        # if using the logical block mask's num columns.\n",
    "\n",
    "        device = block_mask.kv_num_blocks.device\n",
    "\n",
    "        if batch_idx is None:\n",
    "            batch_idx = torch.arange(B, device=device)\n",
    "        page_table = self.page_table[batch_idx]\n",
    "\n",
    "        new_kv_num_blocks = block_mask.kv_num_blocks.clone()\n",
    "\n",
    "        new_kv_indices = torch.zeros((B, H, ROWS, self.n_pages), dtype=torch.int32, device=device)\n",
    "        new_kv_indices[:, :, :, :MAX_BLOCKS_IN_COL] = (\n",
    "            torch.gather(page_table, 1, block_mask.kv_indices.view(B, -1).to(torch.int64))\n",
    "            .view(block_mask.kv_indices.shape)\n",
    "            .to(torch.int32)\n",
    "        )\n",
    "\n",
    "        new_full_kv_indices, new_full_kv_num_blocks = None, None\n",
    "        if block_mask.full_kv_num_blocks is not None:\n",
    "            assert block_mask.full_kv_indices is not None\n",
    "            new_full_kv_num_blocks = block_mask.full_kv_num_blocks.clone()\n",
    "            new_full_kv_indices = torch.zeros(\n",
    "                (B, H, ROWS, self.n_pages), dtype=torch.int32, device=device\n",
    "            )\n",
    "            new_full_kv_indices[:, :, :, :MAX_BLOCKS_IN_COL] = (\n",
    "                torch.gather(\n",
    "                    page_table,\n",
    "                    1,\n",
    "                    block_mask.full_kv_indices.view(B, -1).to(torch.int64),\n",
    "                )\n",
    "                .view(block_mask.full_kv_indices.shape)\n",
    "                .to(torch.int32)\n",
    "            )\n",
    "\n",
    "        new_mask_mod = self.get_mask_mod(block_mask.mask_mod)\n",
    "\n",
    "        seq_lengths = (block_mask.seq_lengths[0], self.n_pages * self.page_size)\n",
    "        return BlockMask.from_kv_blocks(\n",
    "            new_kv_num_blocks,\n",
    "            new_kv_indices,\n",
    "            new_full_kv_num_blocks,\n",
    "            new_full_kv_indices,\n",
    "            block_mask.BLOCK_SIZE,\n",
    "            new_mask_mod,\n",
    "            seq_lengths=seq_lengths,\n",
    "        )\n",
    "\n",
    "    def get_mask_mod(self, mask_mod: Optional[_mask_mod_signature]) -> _mask_mod_signature:\n",
    "        \"\"\"\n",
    "        Converts a mask_mod based on mapping from the physical block index to the logical\n",
    "        block index.\n",
    "\n",
    "        Args:\n",
    "            mask_mod (_mask_mod_signature): mask_mod based on the logical block index.\n",
    "        \"\"\"\n",
    "        if mask_mod is None:\n",
    "            mask_mod = noop_mask\n",
    "\n",
    "        def new_mask_mod(\n",
    "            b: torch.Tensor,\n",
    "            h: torch.Tensor,\n",
    "            q_idx: torch.Tensor,\n",
    "            physical_kv_idx: torch.Tensor,\n",
    "        ):\n",
    "            physical_kv_block = physical_kv_idx // self.page_size\n",
    "            physical_kv_offset = physical_kv_idx % self.page_size\n",
    "            logical_block_idx = self.physical_to_logical[b, physical_kv_block]\n",
    "            logical_kv_idx = logical_block_idx * self.page_size + physical_kv_offset\n",
    "            return torch.where(\n",
    "                logical_block_idx >= 0, mask_mod(b, h, q_idx, logical_kv_idx), False\n",
    "            )\n",
    "\n",
    "        return new_mask_mod\n",
    "\n",
    "    def get_score_mod(self, score_mod: Optional[_score_mod_signature]) -> _score_mod_signature:\n",
    "        \"\"\"\n",
    "        Converts a score_mod based on mapping from the physical block index to the logical\n",
    "        block index.\n",
    "\n",
    "        Args:\n",
    "            score_mod (_score_mod_signature): score_mod based on the logical block index.\n",
    "        \"\"\"\n",
    "        if score_mod is None:\n",
    "            score_mod = _identity\n",
    "\n",
    "        def new_score_mod(\n",
    "            score: torch.Tensor,\n",
    "            b: torch.Tensor,\n",
    "            h: torch.Tensor,\n",
    "            q_idx: torch.Tensor,\n",
    "            physical_kv_idx: torch.Tensor,\n",
    "        ):\n",
    "            physical_kv_block = physical_kv_idx // self.page_size\n",
    "            physical_kv_offset = physical_kv_idx % self.page_size\n",
    "            logical_block_idx = self.physical_to_logical[b, physical_kv_block]\n",
    "            logical_kv_idx = logical_block_idx * self.page_size + physical_kv_offset\n",
    "            return torch.where(\n",
    "                logical_block_idx >= 0,\n",
    "                score_mod(score, b, h, q_idx, logical_kv_idx),\n",
    "                float(\"-inf\"),\n",
    "            )\n",
    "\n",
    "        return new_score_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b874877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRUEvictionPolicy:\n",
    "    def __init__(self):\n",
    "        self.lru_queue = deque()\n",
    "    def register(self, block): self.lru_queue.appendleft(block)\n",
    "    def evict(self): return self.lru_queue.pop()\n",
    "    def unregister(self, block): self.lru_queue.remove(block)\n",
    "\n",
    "    def unregister(self, block):\n",
    "        if block in self.lru_queue:\n",
    "            self.lru_queue.remove(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# ----------------------- Setup -----------------------\n",
    "\n",
    "B = 1\n",
    "H = 2\n",
    "D = 4\n",
    "BLOCK_SIZE = 2\n",
    "NUM_BLOCKS = 4\n",
    "TOTAL_TOKENS = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Simulated token-level key and value tensors\n",
    "k_data = torch.randn(TOTAL_TOKENS, H, D, device=device)\n",
    "v_data = torch.randn_like(k_data)\n",
    "\n",
    "# Flat KV cache buffers for FlexAttention\n",
    "k_cache = torch.zeros(1, H, NUM_BLOCKS * BLOCK_SIZE, D, device=device)\n",
    "v_cache = torch.zeros_like(k_cache)\n",
    "\n",
    "# Page table tensor as expected by FlexAttention\n",
    "page_table_tensor = -torch.ones((B, 8), dtype=torch.long, device=device)\n",
    "\n",
    "# ----------------------- Logical Components -----------------------\n",
    "\n",
    "class LogicalBlock:\n",
    "    def __init__(self, physical_block_id=None):\n",
    "        self.physical_block_id = physical_block_id\n",
    "        self.status = \"gpu\"\n",
    "        self.token_count = 0\n",
    "\n",
    "\n",
    "class PageTable:\n",
    "    def __init__(self):\n",
    "        self.free_list = list(range(NUM_BLOCKS))\n",
    "        self.eviction_policy = LRUEvictionPolicy()\n",
    "    def allocate_block(self, logical_block):\n",
    "        if not self.free_list:\n",
    "            victim = self.eviction_policy.evict()\n",
    "            self.free_list.append(victim.physical_block_id)\n",
    "        pid = self.free_list.pop()\n",
    "        logical_block.physical_block_id = pid\n",
    "        logical_block.status = \"gpu\"\n",
    "        return pid\n",
    "    def swap_to_gpu(self, block): pass  # not needed for this test\n",
    "\n",
    "class KVCacheManager:\n",
    "    def __init__(self, page_table):\n",
    "        self.page_table = page_table\n",
    "        self.sequence_table = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# ----------------------- Test Class -----------------------\n",
    "\n",
    "class SwappablePagedAttention:\n",
    "    def __init__(self, kv_cache_manager, page_size, page_table_tensor, layer_id):\n",
    "        self.kv_cache_manager = kv_cache_manager\n",
    "        self.page_size = page_size\n",
    "        self.page_table = page_table_tensor\n",
    "        self.layer_id = layer_id\n",
    "\n",
    "    def assign(self, batch_idx, input_pos, k_val, v_val, k_cache, v_cache):\n",
    "        B, H, S, D = k_val.shape\n",
    "        for b in range(B):\n",
    "            seq_id = int(batch_idx[b])\n",
    "            for s in range(S):\n",
    "                token_idx = int(input_pos[b, s])\n",
    "                logical_block_id = token_idx // self.page_size\n",
    "                offset = token_idx % self.page_size\n",
    "\n",
    "                block_list = self.kv_cache_manager.sequence_table[seq_id][self.layer_id]\n",
    "                if len(block_list) <= logical_block_id:\n",
    "                    for _ in range(logical_block_id - len(block_list) + 1):\n",
    "                        new_block = LogicalBlock()\n",
    "                        pid = self.kv_cache_manager.page_table.allocate_block(new_block)\n",
    "                        self.kv_cache_manager.page_table.eviction_policy.register(new_block)\n",
    "                        block_list.append(new_block)\n",
    "\n",
    "                block = block_list[logical_block_id]\n",
    "                pid = block.physical_block_id\n",
    "\n",
    "                flat_idx = pid * self.page_size + offset\n",
    "                k_cache[0, :, flat_idx, :] = k_val[b, :, s, :]\n",
    "                v_cache[0, :, flat_idx, :] = v_val[b, :, s, :]\n",
    "\n",
    "                block.token_count += 1\n",
    "                self.page_table[seq_id, logical_block_id] = pid\n",
    "\n",
    "# ----------------------- Run the Test -----------------------\n",
    "\n",
    "pt = PageTable()\n",
    "kvm = KVCacheManager(pt)\n",
    "spa = SwappablePagedAttention(kvm, BLOCK_SIZE, page_table_tensor, layer_id=0)\n",
    "\n",
    "for t in range(TOTAL_TOKENS):\n",
    "    k = k_data[t].unsqueeze(0).unsqueeze(2)  # (1, H, 1, D)\n",
    "    v = v_data[t].unsqueeze(0).unsqueeze(2)\n",
    "    input_pos = torch.tensor([[t]], device=device)\n",
    "    spa.assign(torch.tensor([0], device=device), input_pos, k, v, k_cache, v_cache)\n",
    "\n",
    "# ----------------------- Check Result -----------------------\n",
    "\n",
    "diff = (k_cache[0, :, :TOTAL_TOKENS] - k_data.transpose(0, 1)).abs().max()\n",
    "print(\"Max difference:\", diff.item())  # Expect: 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b72789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build token_index_to_flat_idx mapping\n",
    "token_to_flat_idx = []\n",
    "for t in range(TOTAL_TOKENS):\n",
    "    token_idx = t\n",
    "    logical_block_id = token_idx // BLOCK_SIZE\n",
    "    offset = token_idx % BLOCK_SIZE\n",
    "    pid = int(page_table_tensor[0, logical_block_id].item())\n",
    "    flat_idx = pid * BLOCK_SIZE + offset\n",
    "    token_to_flat_idx.append(flat_idx)\n",
    "\n",
    "# Gather actual written vectors from k_cache\n",
    "actual_k = torch.stack([k_cache[0, :, i, :] for i in token_to_flat_idx], dim=0)  # (T, H, D)\n",
    "expected_k = k_data  # (T, H, D)\n",
    "\n",
    "diff = (actual_k - expected_k).abs().max()\n",
    "print(\"Corrected Max difference:\", diff.item())  # Should be ~0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e8de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageTableWithSwap(PageTable):\n",
    "    def __init__(self, num_blocks, block_size, num_heads, head_dim, device=\"cuda\", k_cache=None, v_cache=None):\n",
    "        super().__init__(num_blocks, block_size, num_heads, head_dim, device=device)\n",
    "        self.k_cache = k_cache  # flat buffer: (1, H, max_tokens, D)\n",
    "        self.v_cache = v_cache\n",
    "        self.cpu_k = {}  # LogicalBlock → CPU tensor\n",
    "        self.cpu_v = {}\n",
    "\n",
    "    def swap_to_cpu(self, block):\n",
    "        pid = block.physical_block_id\n",
    "        if pid is None or block.status != \"gpu\":\n",
    "            raise RuntimeError(\"Block is not on GPU\")\n",
    "\n",
    "        bs = self.block_size\n",
    "        self.cpu_k[block] = self.k_cache[0, :, pid * bs : (pid + 1) * bs, :].clone().cpu()\n",
    "        self.cpu_v[block] = self.v_cache[0, :, pid * bs : (pid + 1) * bs, :].clone().cpu()\n",
    "\n",
    "        block.physical_block_id = None\n",
    "        block.status = \"cpu\"\n",
    "        self.eviction_policy.unregister(block)\n",
    "        self.free_list.append(pid)\n",
    "        print(\"sent\")\n",
    "\n",
    "    def swap_to_gpu(self, block):\n",
    "        if block.status != \"cpu\":\n",
    "            return\n",
    "\n",
    "        if not self.free_list:\n",
    "            victim = self.eviction_policy.evict()\n",
    "            self.swap_to_cpu(victim)\n",
    "\n",
    "        pid = self.free_list.pop()\n",
    "        bs = self.block_size\n",
    "\n",
    "        self.k_cache[0, :, pid * bs : (pid + 1) * bs, :] = self.cpu_k.pop(block).to(self.k_cache.device)\n",
    "        self.v_cache[0, :, pid * bs : (pid + 1) * bs, :] = self.cpu_v.pop(block).to(self.v_cache.device)\n",
    "\n",
    "        block.physical_block_id = pid\n",
    "        block.status = \"gpu\"\n",
    "        self.eviction_policy.register(block)\n",
    "        print(f\"retrieved to pid={pid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Set up environment\n",
    "pt = PageTableWithSwap(\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    num_heads=H,\n",
    "    head_dim=D,\n",
    "    device=device,\n",
    "    k_cache=k_cache,\n",
    "    v_cache=v_cache\n",
    ")\n",
    "kvm = KVCacheManager(pt)\n",
    "spa = SwappablePagedAttention(kvm, BLOCK_SIZE, page_table_tensor, layer_id=0)\n",
    "k_cache.zero_()\n",
    "v_cache.zero_()\n",
    "\n",
    "# ✅ Assign 6 tokens (should fill 3 logical blocks)\n",
    "for t in range(TOTAL_TOKENS):\n",
    "    k = k_data[t].unsqueeze(0).unsqueeze(2)\n",
    "    v = v_data[t].unsqueeze(0).unsqueeze(2)\n",
    "    input_pos = torch.tensor([[t]], device=device)\n",
    "    spa.assign(torch.tensor([0], device=device), input_pos, k, v, k_cache, v_cache)\n",
    "\n",
    "# ✅ Swap out logical block 1 (tokens 2 & 3)\n",
    "evicted_block = kvm.sequence_table[0][0][1]\n",
    "pt.swap_to_cpu(evicted_block, k_cache, v_cache)\n",
    "\n",
    "# ✅ Zero out its region in the flat cache\n",
    "pid = pt.free_list[-1]  # last freed\n",
    "k_cache[0, :, pid * BLOCK_SIZE : (pid + 1) * BLOCK_SIZE, :].zero_()\n",
    "\n",
    "# ✅ Swap it back in\n",
    "pt.swap_to_gpu(evicted_block, k_cache, v_cache)\n",
    "\n",
    "# ✅ Validate restored values match original token 2 and 3\n",
    "flat_idx_2 = evicted_block.physical_block_id * BLOCK_SIZE + 0\n",
    "flat_idx_3 = evicted_block.physical_block_id * BLOCK_SIZE + 1\n",
    "\n",
    "actual_k = torch.stack([\n",
    "    k_cache[0, :, flat_idx_2, :],\n",
    "    k_cache[0, :, flat_idx_3, :]\n",
    "])\n",
    "expected_k = k_data[2:4]\n",
    "\n",
    "diff = (actual_k - expected_k).abs().max()\n",
    "print(\"Restoration diff:\", diff.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca97562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify flex_attention and BlockMask are available from torch\n",
    "try:\n",
    "    from torch.nn.attention.flex_attention import flex_attention, BlockMask\n",
    "    flex_ready = True\n",
    "except ImportError:\n",
    "    flex_ready = False\n",
    "\n",
    "flex_ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention.flex_attention import flex_attention, BlockMask\n",
    "\n",
    "# --- Setup ---\n",
    "B = 1\n",
    "H = 2\n",
    "D = 4\n",
    "SEQ_LEN = 1       # query length\n",
    "BLOCK_SIZE = 2\n",
    "NUM_BLOCKS = 4    # total blocks allocated in k_cache\n",
    "KV_LEN = BLOCK_SIZE * NUM_BLOCKS\n",
    "\n",
    "# Dummy query vector\n",
    "q = torch.randn(B, H, SEQ_LEN, D, device=device)\n",
    "\n",
    "# --- Construct BlockMask to cover all 8 kv slots ---\n",
    "# We must provide all NUM_BLOCKS to match k_cache shape\n",
    "kv_indices = -torch.ones((B, H, 1, NUM_BLOCKS), dtype=torch.int32, device=device)\n",
    "kv_num_blocks = torch.full((B, H, 1), NUM_BLOCKS, dtype=torch.int32, device=device)\n",
    "\n",
    "# Map physical page ids (0 to NUM_BLOCKS-1) as-is\n",
    "for h in range(H):\n",
    "    for i in range(NUM_BLOCKS):\n",
    "        kv_indices[0, h, 0, i] = i\n",
    "\n",
    "# FlexAttention requires BLOCK_SIZE and seq_lengths\n",
    "block_mask = BlockMask.from_kv_blocks(\n",
    "    kv_num_blocks=kv_num_blocks,\n",
    "    kv_indices=kv_indices,\n",
    "    full_kv_num_blocks=None,\n",
    "    full_kv_indices=None,\n",
    "    BLOCK_SIZE=(SEQ_LEN, BLOCK_SIZE),  # (Q, KV) block sizes\n",
    "    mask_mod=None,\n",
    "    seq_lengths=(SEQ_LEN, KV_LEN)\n",
    ")\n",
    "\n",
    "# --- Run FlexAttention ---\n",
    "out = flex_attention(\n",
    "    q,\n",
    "    k_cache,\n",
    "    v_cache,\n",
    "    block_mask=block_mask,\n",
    "    score_mod=None\n",
    ")\n",
    "\n",
    "print(\"Output shape:\", out.shape)       # Expect (1, H, 1, D)\n",
    "print(\"Output vector:\\n\", out[0, :, 0])  # One vector per head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df8525fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention.flex_attention import BlockMask\n",
    "class SwappablePagedAttention:\n",
    "    def __init__(self, kv_cache_manager, k_cache, v_cache, page_table_tensor, page_size, layer_id):\n",
    "        self.kv_cache_manager = kv_cache_manager\n",
    "        self.k_cache = k_cache\n",
    "        self.v_cache = v_cache\n",
    "        self.page_table = page_table_tensor\n",
    "        self.page_size = page_size\n",
    "        self.layer_id = layer_id\n",
    "        self.max_flat_idx_written = 0\n",
    "\n",
    "    # def assign_token(self, seq_id, token_idx, key_vec, value_vec):\n",
    "    #     logical_block_id = token_idx // self.page_size\n",
    "    #     offset = token_idx % self.page_size\n",
    "\n",
    "    #     block_list = self.kv_cache_manager.sequence_table[seq_id][self.layer_id]\n",
    "    #     if len(block_list) <= logical_block_id:\n",
    "    #         for _ in range(logical_block_id - len(block_list) + 1):\n",
    "    #             new_block = LogicalBlock()\n",
    "    #             pid = self.kv_cache_manager.page_table.allocate_block(new_block)\n",
    "    #             self.kv_cache_manager.page_table.eviction_policy.register(new_block)\n",
    "    #             block_list.append(new_block)\n",
    "\n",
    "    #     block = block_list[logical_block_id]\n",
    "    #     pid = block.physical_block_id\n",
    "\n",
    "    #     # flat_idx = pid * self.page_size + offset\n",
    "    #     # self.k_cache[0, :, flat_idx, :] = key_vec\n",
    "    #     # self.v_cache[0, :, flat_idx, :] = value_vec\n",
    "\n",
    "    #     flat_idx = pid * self.page_size + offset\n",
    "    #     self.max_flat_idx_written = max(self.max_flat_idx_written, flat_idx + 1)\n",
    "    #     self.k_cache[0, :, flat_idx, :] = key_vec.squeeze(1)  # (H, D)\n",
    "    #     self.v_cache[0, :, flat_idx, :] = value_vec.squeeze(1)\n",
    "\n",
    "\n",
    "    #     block.token_count += 1\n",
    "    #     self.page_table[seq_id, logical_block_id] = pid\n",
    "\n",
    "    def assign_token(self, seq_id, token_idx, key_vec, value_vec):\n",
    "        logical_block_id = token_idx // self.page_size\n",
    "        offset = token_idx % self.page_size\n",
    "\n",
    "        # # ⬇️ Expand page_table if seq_id exceeds current rows\n",
    "        # if seq_id >= self.page_table.shape[0]:\n",
    "        #     pad = seq_id - self.page_table.shape[0] + 1\n",
    "        #     new_table = torch.full(\n",
    "        #         (self.page_table.shape[0] + pad, self.page_table.shape[1]),\n",
    "        #         fill_value=-1,\n",
    "        #         dtype=self.page_table.dtype,\n",
    "        #         device=self.page_table.device\n",
    "        #     )\n",
    "        #     new_table[:self.page_table.shape[0], :] = self.page_table\n",
    "        #     self.page_table = new_table\n",
    "\n",
    "\n",
    "        # ⬇️ Expand page_table if the logical block id exceeds current columns\n",
    "        if logical_block_id >= self.page_table.shape[1]:\n",
    "            pad = logical_block_id - self.page_table.shape[1] + 1\n",
    "            new_table = torch.full(\n",
    "                (self.page_table.shape[0], self.page_table.shape[1] + pad),\n",
    "                fill_value=-1,\n",
    "                dtype=self.page_table.dtype,\n",
    "                device=self.page_table.device\n",
    "            )\n",
    "            new_table[:, :self.page_table.shape[1]] = self.page_table\n",
    "            self.page_table = new_table\n",
    "\n",
    "        # Get or grow the block list for this sequence/layer\n",
    "        block_list = self.kv_cache_manager.sequence_table[seq_id][self.layer_id]\n",
    "        if len(block_list) <= logical_block_id:\n",
    "            for _ in range(logical_block_id - len(block_list) + 1):\n",
    "                new_block = LogicalBlock()\n",
    "                pid = self.kv_cache_manager.page_table.allocate_block(new_block)\n",
    "                self.kv_cache_manager.page_table.eviction_policy.register(new_block)\n",
    "                block_list.append(new_block)\n",
    "\n",
    "        block = block_list[logical_block_id]\n",
    "\n",
    "        # If it's not on GPU, swap it in\n",
    "        if block.status == \"cpu\":\n",
    "            self.kv_cache_manager.page_table.swap_to_gpu(block)\n",
    "\n",
    "        pid = block.physical_block_id\n",
    "        # Sanity check:\n",
    "        if block.physical_block_id is None:\n",
    "            raise RuntimeError(f\"Token {token_idx}: block still has no physical_block_id after swap.\")\n",
    "\n",
    "        flat_idx = pid * self.page_size + offset\n",
    "\n",
    "        self.k_cache[0, :, flat_idx, :] = key_vec.squeeze(1)\n",
    "        self.v_cache[0, :, flat_idx, :] = value_vec.squeeze(1)\n",
    "\n",
    "        block.token_count += 1\n",
    "        if seq_id >= self.page_table.shape[0] or logical_block_id >= self.page_table.shape[1]:\n",
    "            print(f\"🚨 Resize needed: seq_id={seq_id}, logical_block_id={logical_block_id}, current shape={self.page_table.shape}\")\n",
    "\n",
    "        self.page_table[seq_id, logical_block_id] = pid\n",
    "\n",
    "        self.max_flat_idx_written = max(self.max_flat_idx_written, flat_idx + 1)\n",
    "\n",
    "\n",
    "\n",
    "    # def build_blockmask(self, num_query_tokens, total_blocks):\n",
    "    #     from torch.nn.attention.flex_attention import BlockMask\n",
    "    #     B, H = 1, self.k_cache.shape[1]\n",
    "\n",
    "    #     kv_indices = -torch.ones((B, H, 1, total_blocks), dtype=torch.int32, device=self.k_cache.device)\n",
    "    #     kv_num_blocks = torch.full((B, H, 1), total_blocks, dtype=torch.int32, device=self.k_cache.device)\n",
    "\n",
    "    #     for h in range(H):\n",
    "    #         for i in range(total_blocks):\n",
    "    #             kv_indices[0, h, 0, i] = i\n",
    "\n",
    "    #     block_mask = BlockMask.from_kv_blocks(\n",
    "    #         kv_num_blocks=kv_num_blocks,\n",
    "    #         kv_indices=kv_indices,\n",
    "    #         full_kv_num_blocks=None,\n",
    "    #         full_kv_indices=None,\n",
    "    #         BLOCK_SIZE=(num_query_tokens, self.page_size),\n",
    "    #         mask_mod=None,\n",
    "    #         seq_lengths=(num_query_tokens, total_blocks * self.page_size)\n",
    "    #     )\n",
    "    #     return block_mask\n",
    "    \n",
    "    # def build_blockmask(self, num_query_tokens, total_tokens_written):\n",
    "    #     from torch.nn.attention.flex_attention import BlockMask\n",
    "        \n",
    "    #     # assert total_tokens_written <= self.k_cache.shape[2], \"KV length exceeds cache capacity\"\n",
    "    #     B, H = 1, self.k_cache.shape[1]\n",
    "    #     device = self.k_cache.device\n",
    "\n",
    "    #     logical_to_physical = self.page_table[0]\n",
    "    #     valid = logical_to_physical != -1\n",
    "    #     pids = logical_to_physical[valid]\n",
    "    #     total_blocks = pids.numel()\n",
    "\n",
    "    #     kv_indices = -torch.ones((B, H, 1, total_blocks), dtype=torch.int32, device=device)\n",
    "    #     kv_num_blocks = torch.full((B, H, 1), total_blocks, dtype=torch.int32, device=device)\n",
    "\n",
    "    #     for h in range(H):\n",
    "    #         for i in range(total_blocks):\n",
    "    #             kv_indices[0, h, 0, i] = pids[i].item()\n",
    "\n",
    "\n",
    "    #     # print(\"max pid:\", max(pids))\n",
    "    #     # print(\"kv_num_blocks:\", kv_num_blocks)\n",
    "    #     # print(\"kv_indices:\", kv_indices)\n",
    "    #     # assert (kv_indices >= 0).all(), \"Found -1 in kv_indices\"\n",
    "    #     # assert (kv_indices < kv_num_blocks.max()).all(), \"kv_indices out of bounds\"\n",
    "\n",
    "\n",
    "    #     block_mask = BlockMask.from_kv_blocks(\n",
    "    #         kv_num_blocks=kv_num_blocks,\n",
    "    #         kv_indices=kv_indices,\n",
    "    #         full_kv_num_blocks=None,\n",
    "    #         full_kv_indices=None,\n",
    "    #         BLOCK_SIZE=(num_query_tokens, self.page_size),\n",
    "    #         mask_mod=None,\n",
    "    #         seq_lengths=(num_query_tokens, self.max_flat_idx_written) # seq_lengths=(num_query_tokens, total_tokens_written)\n",
    "    #     )\n",
    "\n",
    "    #     # ✅ Crop to match actual (query, kv) shapes\n",
    "    #     return block_mask._adjust(num_query_tokens, total_tokens_written)\n",
    "    \n",
    "    def build_blockmask(self, num_query_tokens, total_tokens_written):\n",
    "        from torch.nn.attention.flex_attention import BlockMask\n",
    "\n",
    "        assert total_tokens_written <= self.k_cache.shape[2], \"KV length exceeds cache capacity\"\n",
    "        B, H = 1, self.k_cache.shape[1]\n",
    "        device = self.k_cache.device\n",
    "\n",
    "        logical_to_physical = self.page_table[0]  # single seq_id only\n",
    "        valid = logical_to_physical != -1\n",
    "        pids = logical_to_physical[valid]\n",
    "        total_blocks = pids.numel()\n",
    "\n",
    "        kv_indices = torch.full((B, H, 1, total_blocks), -1, dtype=torch.int32, device=device)\n",
    "        kv_num_blocks = torch.full((B, H, 1), total_blocks, dtype=torch.int32, device=device)\n",
    "\n",
    "        for h in range(H):\n",
    "            for i in range(total_blocks):\n",
    "                kv_indices[0, h, 0, i] = pids[i].item()\n",
    "        print(\">> max_flat_idx_written:\", self.max_flat_idx_written)\n",
    "        print(\">> kv_num_blocks.shape:\", kv_num_blocks.shape)\n",
    "        print(\">> kv_num_blocks:\", kv_num_blocks)\n",
    "        print(\">> kv_indices.shape:\", kv_indices.shape)\n",
    "        print(\">> kv_indices:\", kv_indices)\n",
    "\n",
    "        block_mask = BlockMask.from_kv_blocks(\n",
    "            kv_num_blocks=kv_num_blocks,\n",
    "            kv_indices=kv_indices,\n",
    "            full_kv_num_blocks=None,\n",
    "            full_kv_indices=None,\n",
    "            BLOCK_SIZE=(num_query_tokens, self.page_size),\n",
    "            mask_mod=None,\n",
    "            seq_lengths=(num_query_tokens, total_tokens_written)  # <- exact KV length\n",
    "        )\n",
    "\n",
    "        # Crop block mask to match token counts\n",
    "        return block_mask._adjust(num_query_tokens, total_tokens_written)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def query(self, q, block_mask):\n",
    "    #     from torch.nn.attention.flex_attention import flex_attention\n",
    "    #     return flex_attention(\n",
    "    #         q, self.k_cache, self.v_cache,\n",
    "    #         block_mask=block_mask,\n",
    "    #         score_mod=None\n",
    "    #     )\n",
    "    def query(self, q, block_mask):\n",
    "        from torch.nn.attention.flex_attention import flex_attention\n",
    "\n",
    "        # 🔁 Ensure required blocks are on GPU\n",
    "        for block_list in self.kv_cache_manager.sequence_table.values():\n",
    "            for block in block_list[self.layer_id]:\n",
    "                if block.status == \"cpu\":\n",
    "                    self.kv_cache_manager.page_table.swap_to_gpu(block)\n",
    "                    print(f\"⏪ Swapping in block with pid={block.physical_block_id}\")\n",
    "\n",
    "        print(\"Calling flex_attention with:\")\n",
    "        print(\"  q.shape:\", q.shape)\n",
    "        print(\"  k.shape:\", self.k_cache.shape)\n",
    "        print(\"  v.shape:\", self.v_cache.shape)\n",
    "\n",
    "\n",
    "        return flex_attention(\n",
    "            q, self.k_cache, self.v_cache,\n",
    "            block_mask=block_mask,\n",
    "            score_mod=None\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff965eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup the full KVCacheManager + PageTableWithSwap\n",
    "pt = PageTableWithSwap(\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    num_heads=H,\n",
    "    head_dim=D,\n",
    "    device=device,\n",
    "    k_cache=k_cache,\n",
    "    v_cache=v_cache\n",
    ")\n",
    "kvm = KVCacheManager(pt)\n",
    "\n",
    "# 2. Flat cache buffers\n",
    "k_cache = torch.zeros(1, H, NUM_BLOCKS * BLOCK_SIZE, D, device=device)\n",
    "v_cache = torch.zeros_like(k_cache)\n",
    "page_table_tensor = -torch.ones((1, NUM_BLOCKS), dtype=torch.long, device=device)\n",
    "\n",
    "# 3. Swappable attention wrapper\n",
    "spa = SwappablePagedAttention(kvm, k_cache, v_cache, page_table_tensor, page_size=BLOCK_SIZE, layer_id=0)\n",
    "\n",
    "# 4. Assign 6 tokens (token 0–5) using full KVCacheManager\n",
    "for t in range(6):\n",
    "    spa.assign_token(seq_id=0, token_idx=t, key_vec=k_data[t], value_vec=v_data[t])\n",
    "\n",
    "# 5. Evict logical block 1 (tokens 2–3)\n",
    "evicted_block = kvm.sequence_table[0][0][1]\n",
    "pt.swap_to_cpu(evicted_block)\n",
    "\n",
    "# Zero out that region in the flat cache\n",
    "evicted_pid = pt.free_list[-1]\n",
    "k_cache[0, :, evicted_pid * BLOCK_SIZE : (evicted_pid + 1) * BLOCK_SIZE, :] = 0\n",
    "\n",
    "# 6. Restore the block\n",
    "pt.swap_to_gpu(evicted_block)\n",
    "\n",
    "# 7. Build a query vector\n",
    "q = torch.randn(1, H, 1, D, device=device)\n",
    "\n",
    "# 8. Build a block mask for all 4 physical blocks (0–3)\n",
    "block_mask = spa.build_blockmask(num_query_tokens=1, total_blocks=NUM_BLOCKS)\n",
    "\n",
    "# 9. Run flex attention\n",
    "out = spa.query(q, block_mask)\n",
    "\n",
    "# 10. Print\n",
    "print(\"FlexAttention output after swap/restore:\", out[0, :, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2edb2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SwappablePagedAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, head_dim, dtype, spa: SwappablePagedAttention):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.spa = spa\n",
    "\n",
    "        self.wqkv = nn.Linear(hidden_dim, 3 * hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        self.wo = nn.Linear(hidden_dim, hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "\n",
    "        self.block_size = spa.page_size\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            seq_len=512,  # max positions\n",
    "            n_elem=self.head_dim,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, seq_id, token_idx):\n",
    "        B, S, _ = x.shape\n",
    "        kv_size = self.n_heads * self.head_dim\n",
    "\n",
    "        q, k, v = self.wqkv(x).split([kv_size, kv_size, kv_size], dim=-1)\n",
    "        q = q.view(B, S, self.n_heads, self.head_dim)\n",
    "        k = k.view(B, S, self.n_heads, self.head_dim)\n",
    "        v = v.view(B, S, self.n_heads, self.head_dim)\n",
    "\n",
    "        freqs = self.freqs_cis[token_idx].unsqueeze(0)  # (1, 1, D//2, 2)\n",
    "        q = apply_rotary_emb(q, freqs)\n",
    "        k = apply_rotary_emb(k, freqs)\n",
    "\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)  # (B, H, S, D)\n",
    "\n",
    "        # assign KV\n",
    "        self.spa.assign_token(seq_id, token_idx, k[0], v[0])\n",
    "\n",
    "        # build blockmask for this query\n",
    "        block_list = self.spa.kv_cache_manager.sequence_table[seq_id][self.spa.layer_id]\n",
    "        num_blocks = len(block_list)\n",
    "        # block_mask = self.spa.build_blockmask(num_query_tokens=1, total_blocks=num_blocks)\n",
    "        block_mask = self.spa.build_blockmask(num_query_tokens=1, total_tokens_written=self.spa.max_flat_idx_written) # token_idx + 1)\n",
    "\n",
    "        # run attention\n",
    "        out = self.spa.query(q, block_mask)  # (B, H, 1, D)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, S, -1)\n",
    "\n",
    "        return self.wo(out)\n",
    "        \n",
    "def precompute_freqs_cis(seq_len, n_elem, base=10000, dtype=torch.float16):\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem))\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1).to(dtype=dtype, device=\"cuda\")\n",
    "\n",
    "def apply_rotary_emb(x, freqs_cis):\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    freqs_cis = freqs_cis.view(xshaped.size(0), xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack([\n",
    "        xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
    "        xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
    "    ], dim=-1)\n",
    "    return x_out2.flatten(3).type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea565605",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cache = torch.zeros(\n",
    "    1, H, NUM_BLOCKS * BLOCK_SIZE, D,\n",
    "    device=device, dtype=torch.float16  # ← match the model/layer dtype\n",
    ")\n",
    "v_cache = torch.zeros_like(k_cache)\n",
    "pt = PageTableWithSwap(\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    num_heads=H,\n",
    "    head_dim=D,\n",
    "    device=device,\n",
    "    k_cache=k_cache,\n",
    "    v_cache=v_cache\n",
    ")\n",
    "kvm = KVCacheManager(pt)\n",
    "spa = SwappablePagedAttention(kvm, k_cache, v_cache, page_table_tensor, BLOCK_SIZE, layer_id=0)\n",
    "layer = SwappablePagedAttentionLayer(hidden_dim=D*H, n_heads=H, head_dim=D, dtype=torch.float16, spa=spa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53674596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent\n",
      "retrieved to pid=1\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[10],\n",
      "         [10]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 10])\n",
      ">> kv_indices: tensor([[[[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "Layer output: torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, D * H, device=device, dtype=torch.float16)  # (B, 1, D_model)\n",
    "out = layer(x, seq_id=0, token_idx=6)\n",
    "\n",
    "print(\"Layer output:\", out.shape)  # expect (1, 1, D_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544101f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, D * H, device=device, dtype=torch.float16)\n",
    "out = layer(x, seq_id=0, token_idx=6)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1094e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 4\n",
    "NUM_BLOCKS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1abafa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> max_flat_idx_written: 5\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 6\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 7\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "sent\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 1]],\n",
      "\n",
      "         [[1, 0, 1]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 1]],\n",
      "\n",
      "         [[1, 0, 1]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 3])\n",
      ">> kv_indices: tensor([[[[1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "sent\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[4],\n",
      "         [4]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 4])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[4],\n",
      "         [4]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 4])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[4],\n",
      "         [4]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 4])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[4],\n",
      "         [4]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 4])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "sent\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[5],\n",
      "         [5]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 5])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 1]],\n",
      "\n",
      "         [[1, 0, 0, 0, 1]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[5],\n",
      "         [5]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 5])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[5],\n",
      "         [5]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 5])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 1]],\n",
      "\n",
      "         [[1, 0, 0, 0, 1]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[5],\n",
      "         [5]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 5])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "sent\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[6],\n",
      "         [6]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 6])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[6],\n",
      "         [6]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 6])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[6],\n",
      "         [6]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 6])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[6],\n",
      "         [6]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 6])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "sent\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[7],\n",
      "         [7]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 7])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 1]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[7],\n",
      "         [7]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 7])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[7],\n",
      "         [7]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 7])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 1]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[7],\n",
      "         [7]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 7])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "sent\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[8],\n",
      "         [8]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 8])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[8],\n",
      "         [8]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 8])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[8],\n",
      "         [8]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 8])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[8],\n",
      "         [8]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 8])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "sent\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[9],\n",
      "         [9]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 9])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0, 1]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[9],\n",
      "         [9]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 9])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[9],\n",
      "         [9]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 9])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0, 1]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[9],\n",
      "         [9]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 9])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "sent\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[10],\n",
      "         [10]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 10])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[10],\n",
      "         [10]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 10])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[10],\n",
      "         [10]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 10])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      ">> max_flat_idx_written: 8\n",
      ">> kv_num_blocks.shape: torch.Size([1, 2, 1])\n",
      ">> kv_num_blocks: tensor([[[10],\n",
      "         [10]]], device='cuda:0', dtype=torch.int32)\n",
      ">> kv_indices.shape: torch.Size([1, 2, 1, 10])\n",
      ">> kv_indices: tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 8, 4])\n",
      "  v.shape: torch.Size([1, 2, 8, 4])\n",
      "Final output shape: torch.Size([1, 40, 8])\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "seq_id = 0\n",
    "T = 40  # number of tokens to decode\n",
    "hidden_dim = D * H\n",
    "\n",
    "# --- Token input embeddings (e.g., dummy input tokens) ---\n",
    "token_inputs = torch.randn(T, 1, hidden_dim, device=device, dtype=torch.float16)  # (T, B=1, D_model)\n",
    "\n",
    "# --- Create a fresh attention layer per run ---\n",
    "layer = SwappablePagedAttentionLayer(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=H,\n",
    "    head_dim=D,\n",
    "    dtype=torch.float16,\n",
    "    spa=spa\n",
    ")\n",
    "\n",
    "# --- Run the decoding loop ---\n",
    "outputs = []\n",
    "\n",
    "for t in range(T):\n",
    "    x_t = token_inputs[t].unsqueeze(0)  # (1, 1, D_model)\n",
    "    out_t = layer(x_t, seq_id=seq_id, token_idx=t)  # (1, 1, D_model)\n",
    "    outputs.append(out_t)\n",
    "\n",
    "# --- Stack outputs ---\n",
    "outputs = torch.cat(outputs, dim=1)  # (1, T, D_model)\n",
    "\n",
    "print(\"Final output shape:\", outputs.shape)  # Expect: (1, T, D_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffc1cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.shape: torch.Size([2, 1, 1, 4])\n",
      "k.shape: torch.Size([2, 1, 8, 4])\n",
      "v.shape: torch.Size([2, 1, 8, 4])\n",
      "block_mask.shape: (2, 1, 1, 8)\n",
      "output.shape: torch.Size([2, 1, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.attention.flex_attention import flex_attention, BlockMask\n",
    "\n",
    "# --- Dummy config ---\n",
    "B, H, S, D = 2, 1, 1, 4             # batch=2, heads=1, q_len=1, head_dim=4\n",
    "num_kv_tokens = 8\n",
    "block_size = 1                     # keep it simple: 1 token per block\n",
    "num_kv_blocks = num_kv_tokens // block_size\n",
    "\n",
    "# --- Dummy Q, K, V ---\n",
    "q = torch.randn(B, H, S, D, device='cuda')                  # (2, 1, 1, 4)\n",
    "k = torch.randn(B, H, num_kv_tokens, D, device='cuda')      # (2, 1, 8, 4)\n",
    "v = torch.randn(B, H, num_kv_tokens, D, device='cuda')      # (2, 1, 8, 4)\n",
    "\n",
    "# --- Simulate page tables ---\n",
    "# For each sequence ID in batch, assign logical blocks to physical blocks\n",
    "# e.g., both sequence 0 and 1 map to [0, 1, 2, 3, 4, 5, 6, 7] (identity)\n",
    "logical_to_physical = torch.arange(num_kv_blocks, device='cuda')\n",
    "dummy_page_table = [logical_to_physical.clone() for _ in range(B)]  # One per batch\n",
    "\n",
    "# --- Build BlockMask ---\n",
    "kv_num_blocks = torch.full((B, H, 1), num_kv_blocks, dtype=torch.int32, device='cuda')  # (2, 1, 1)\n",
    "kv_indices = torch.full((B, H, 1, num_kv_blocks), -1, dtype=torch.int32, device='cuda') # (2, 1, 1, 8)\n",
    "\n",
    "for b in range(B):\n",
    "    for i in range(num_kv_blocks):\n",
    "        kv_indices[b, 0, 0, i] = dummy_page_table[b][i].item()\n",
    "\n",
    "block_mask = BlockMask.from_kv_blocks(\n",
    "    kv_num_blocks=kv_num_blocks,\n",
    "    kv_indices=kv_indices,\n",
    "    full_kv_num_blocks=None,\n",
    "    full_kv_indices=None,\n",
    "    BLOCK_SIZE=(S, block_size),\n",
    "    mask_mod=None,\n",
    "    seq_lengths=(S, num_kv_tokens),\n",
    ")._adjust(S, num_kv_tokens)\n",
    "\n",
    "# --- Run FlexAttention ---\n",
    "out = flex_attention(q, k, v, block_mask=block_mask)\n",
    "\n",
    "# --- Verify ---\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"block_mask.shape:\", block_mask.shape)\n",
    "print(\"output.shape:\", out.shape)  # should be (2, 1, 1, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd577fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlockMask shape: (2, 2, 1, 8)\n",
      "KV Num Blocks:\n",
      " tensor([[[3],\n",
      "         [3]],\n",
      "\n",
      "        [[3],\n",
      "         [3]]], device='cuda:0', dtype=torch.int32)\n",
      "KV Indices (seq 0, head 0): tensor([1, 0, 2], device='cuda:0', dtype=torch.int32)\n",
      "KV Indices (seq 1, head 0): tensor([3, 4, 5], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Dummy BlockMask that mimics FlexAttention behavior\n",
    "class DummyBlockMask:\n",
    "    def __init__(self, kv_num_blocks, kv_indices, shape):\n",
    "        self.kv_num_blocks = kv_num_blocks\n",
    "        self.kv_indices = kv_indices\n",
    "        self._shape = shape\n",
    "\n",
    "    def _adjust(self, q_len, kv_len):\n",
    "        self._shape = (self.kv_num_blocks.shape[0], self.kv_num_blocks.shape[1], q_len, kv_len)\n",
    "        return self\n",
    "\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "\n",
    "\n",
    "def build_blockmask_batched(page_tables, seq_ids, num_query_tokens, total_tokens_written, page_size):\n",
    "    B = len(seq_ids)\n",
    "    H = 2  # number of heads\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    kv_num_blocks = []\n",
    "    kv_indices = []\n",
    "\n",
    "    for seq_id in seq_ids:\n",
    "        page_table = page_tables[seq_id]  # 1D tensor: logical → physical block id\n",
    "        valid = page_table != -1\n",
    "        pids = page_table[valid]\n",
    "        total_blocks = pids.numel()\n",
    "\n",
    "        kv_num_blocks.append(torch.full((H, 1), total_blocks, dtype=torch.int32, device=device))\n",
    "        pid_tensor = torch.full((H, 1, total_blocks), -1, dtype=torch.int32, device=device)\n",
    "        for h in range(H):\n",
    "            pid_tensor[h, 0, :total_blocks] = pids\n",
    "        kv_indices.append(pid_tensor)\n",
    "\n",
    "    kv_num_blocks = torch.stack(kv_num_blocks, dim=0)  # (B, H, 1)\n",
    "    kv_indices = torch.stack(kv_indices, dim=0)        # (B, H, 1, T)\n",
    "\n",
    "    block_mask = DummyBlockMask(kv_num_blocks, kv_indices, kv_indices.shape)\n",
    "    return block_mask._adjust(num_query_tokens, total_tokens_written), kv_num_blocks, kv_indices\n",
    "\n",
    "\n",
    "# --- Test the blockmask ---\n",
    "B = 2\n",
    "T = 8\n",
    "page_size = 1\n",
    "seq_ids = list(range(B))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dummy page tables\n",
    "page_tables = {\n",
    "    0: torch.tensor([1, 0, 2, -1, -1, -1, -1, -1], dtype=torch.int32, device=device),\n",
    "    1: torch.tensor([3, 4, 5, -1, -1, -1, -1, -1], dtype=torch.int32, device=device),\n",
    "}\n",
    "\n",
    "# Build and test blockmask\n",
    "block_mask, kv_num_blocks, kv_indices = build_blockmask_batched(\n",
    "    page_tables=page_tables,\n",
    "    seq_ids=seq_ids,\n",
    "    num_query_tokens=1,\n",
    "    total_tokens_written=T,\n",
    "    page_size=page_size,\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"BlockMask shape:\", block_mask._shape)\n",
    "print(\"KV Num Blocks:\\n\", kv_num_blocks)\n",
    "print(\"KV Indices (seq 0, head 0):\", kv_indices[0, 0, 0])\n",
    "print(\"KV Indices (seq 1, head 0):\", kv_indices[1, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedb5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.shape[-2]: 16\n",
      "total_tokens_written: 16\n",
      "kv_indices: tensor([[[[ 0,  1,  2,  3,  4,  5,  6,  7, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "         [[ 0,  1,  2,  3,  4,  5,  6,  7, -1, -1, -1, -1, -1, -1, -1, -1]]],\n",
      "\n",
      "\n",
      "        [[[ 8,  9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "         [[ 8,  9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1]]]],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "kv_indices.max(): 15\n",
      "total_tokens_written: 16\n",
      "q.shape: torch.Size([2, 2, 1, 4])\n",
      "k.shape: torch.Size([2, 2, 16, 4])\n",
      "block_mask.shape: (2, 2, 1, 16)\n",
      "output.shape: torch.Size([2, 2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.attention.flex_attention import flex_attention, BlockMask\n",
    "\n",
    "# --- Config ---\n",
    "B, H, S, D = 2, 2, 1, 4  # 2 sequences, 2 heads, 1 query token, head_dim=4\n",
    "page_size = 1\n",
    "KV = 16  # total kv tokens\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# --- Dummy QKV ---\n",
    "q = torch.randn(B, H, S, D, device=device)\n",
    "k = torch.randn(B, H, KV, D, device=device)\n",
    "v = torch.randn(B, H, KV, D, device=device)\n",
    "\n",
    "# --- Dummy page tables (NO -1s here!) ---\n",
    "page_tables = {\n",
    "    0: torch.tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32, device=device),\n",
    "    1: torch.tensor([8, 9, 10, 11, 12, 13, 14, 15], dtype=torch.int32, device=device),\n",
    "}\n",
    "\n",
    "seq_ids = list(page_tables.keys())\n",
    "\n",
    "# --- Blockmask builder ---\n",
    "def build_blockmask_batched(page_tables, seq_ids, num_query_tokens, total_tokens_written, page_size):\n",
    "    B = len(seq_ids)\n",
    "    H = 2  # number of heads\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # max_blocks = max((pt != -1).sum().item() for pt in page_tables.values())\n",
    "    max_blocks = total_tokens_written // page_size  # = 16 // 1 = 16\n",
    "\n",
    "\n",
    "    kv_num_blocks = torch.full((B, H, 1), 0, dtype=torch.int32, device=device)\n",
    "    kv_indices = torch.full((B, H, 1, max_blocks), -1, dtype=torch.int32, device=device)\n",
    "\n",
    "    for b, seq_id in enumerate(seq_ids):\n",
    "        l2p = page_tables[seq_id]\n",
    "        valid = l2p != -1\n",
    "        pids = l2p[valid]\n",
    "        T = pids.numel()\n",
    "\n",
    "        kv_num_blocks[b] = torch.full((H, 1), T, dtype=torch.int32, device=device)\n",
    "        for h in range(H):\n",
    "            kv_indices[b, h, 0, :T] = pids \n",
    "\n",
    "    # Safety checks\n",
    "    assert (kv_indices >= -1).all(), \"kv_indices contain values < -1!\"\n",
    "    # assert (kv_indices < total_tokens_written).all(), \"kv_indices out-of-bounds!\"\n",
    "    assert (kv_indices < total_tokens_written).all() | (kv_indices == -1).all()\n",
    "    assert kv_indices.max().item() < total_tokens_written, \"kv_indices contains out-of-bound index!\"\n",
    "\n",
    "\n",
    "    print(\"K.shape[-2]:\", k.shape[-2])\n",
    "    print(\"total_tokens_written:\", KV)\n",
    "\n",
    "    print(\"kv_indices:\", kv_indices)\n",
    "    print(\"kv_indices.max():\", kv_indices.max().item())\n",
    "    print(\"total_tokens_written:\", total_tokens_written)\n",
    "\n",
    "    block_mask = BlockMask.from_kv_blocks(\n",
    "        kv_num_blocks=kv_num_blocks,\n",
    "        kv_indices=kv_indices,\n",
    "        full_kv_num_blocks=None,\n",
    "        full_kv_indices=None,\n",
    "        BLOCK_SIZE=(num_query_tokens, page_size),\n",
    "        mask_mod=None,\n",
    "        seq_lengths=(num_query_tokens, total_tokens_written),\n",
    "    )\n",
    "\n",
    "    return block_mask\n",
    "    # return block_mask._adjust(num_query_tokens, total_tokens_written)\n",
    "\n",
    "\n",
    "# --- Build mask ---\n",
    "block_mask = build_blockmask_batched(\n",
    "    page_tables=page_tables,\n",
    "    seq_ids=seq_ids,\n",
    "    num_query_tokens=q.shape[2],\n",
    "    total_tokens_written=k.shape[2],\n",
    "    page_size=page_size,\n",
    ")\n",
    "\n",
    "# --- Call flex_attention ---\n",
    "out = flex_attention(q, k, v, block_mask=block_mask)\n",
    "\n",
    "# --- Done ---\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"block_mask.shape:\", block_mask.shape)\n",
    "print(\"output.shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f40d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Helps surface actual error sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c481ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 1 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 2 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 3 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 4 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 5 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 6 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 7 | out.shape: torch.Size([2, 2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.attention.flex_attention import flex_attention, BlockMask\n",
    "\n",
    "# --- Config ---\n",
    "B, H, S, D = 2, 2, 1, 4\n",
    "page_size = 1\n",
    "max_decode_len = 8  # decode 8 tokens per sequence\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# --- Allocate flat K/V cache ---\n",
    "KV = B * max_decode_len\n",
    "k_cache = torch.zeros((1, H, KV, D), device=device)\n",
    "v_cache = torch.zeros((1, H, KV, D), device=device)\n",
    "\n",
    "# --- Initialize empty page_table and sequence_table ---\n",
    "page_table = torch.full((B, max_decode_len), -1, dtype=torch.int32, device=device)\n",
    "sequence_table = {i: [] for i in range(B)}\n",
    "\n",
    "# --- LogicalBlock class for test ---\n",
    "class LogicalBlock:\n",
    "    def __init__(self, pid):\n",
    "        self.physical_block_id = pid\n",
    "        self.token_count = 0\n",
    "        self.status = \"gpu\"\n",
    "\n",
    "# --- Assign token ---\n",
    "def assign_token(seq_id, token_idx, k_vec, v_vec):\n",
    "    logical_block_id = token_idx // page_size\n",
    "    offset = token_idx % page_size\n",
    "    pid = seq_id * max_decode_len + token_idx  # flat block assignment (mock logic)\n",
    "\n",
    "    # Record block if first time\n",
    "    if len(sequence_table[seq_id]) <= logical_block_id:\n",
    "        block = LogicalBlock(pid)\n",
    "        sequence_table[seq_id].append(block)\n",
    "\n",
    "    flat_idx = pid * page_size + offset\n",
    "    k_cache[0, :, flat_idx, :] = k_vec.squeeze(1)\n",
    "    v_cache[0, :, flat_idx, :] = v_vec.squeeze(1)\n",
    "\n",
    "    page_table[seq_id, logical_block_id] = pid\n",
    "    sequence_table[seq_id][logical_block_id].token_count += 1\n",
    "\n",
    "\n",
    "# --- Build blockmask ---\n",
    "def build_blockmask_batched(page_tables, seq_ids, num_query_tokens, total_tokens_written, page_size):\n",
    "    B = len(seq_ids)\n",
    "    H = k_cache.shape[1]\n",
    "    max_blocks = total_tokens_written // page_size\n",
    "    device = k_cache.device\n",
    "\n",
    "    kv_num_blocks = torch.full((B, H, 1), 0, dtype=torch.int32, device=device)\n",
    "    kv_indices = torch.full((B, H, 1, max_blocks), -1, dtype=torch.int32, device=device)\n",
    "\n",
    "    for b, seq_id in enumerate(seq_ids):\n",
    "        l2p = page_tables[seq_id]\n",
    "        valid = l2p != -1\n",
    "        pids = l2p[valid]\n",
    "        T = pids.numel()\n",
    "\n",
    "        kv_num_blocks[b] = torch.full((H, 1), T, dtype=torch.int32, device=device)\n",
    "        for h in range(H):\n",
    "            kv_indices[b, h, 0, :T] = pids\n",
    "\n",
    "    block_mask = BlockMask.from_kv_blocks(\n",
    "        kv_num_blocks=kv_num_blocks,\n",
    "        kv_indices=kv_indices,\n",
    "        full_kv_num_blocks=None,\n",
    "        full_kv_indices=None,\n",
    "        BLOCK_SIZE=(num_query_tokens, page_size),\n",
    "        mask_mod=None,\n",
    "        seq_lengths=(num_query_tokens, total_tokens_written),\n",
    "    )\n",
    "\n",
    "    return block_mask\n",
    "\n",
    "\n",
    "# --- Decode loop ---\n",
    "for t in range(max_decode_len):\n",
    "    x = torch.randn(B, S, 3*H * D, device=device)\n",
    "    q, k, v = x.split([H*D, H*D, H*D], dim=-1)\n",
    "    q = q.view(B, S, H, D).transpose(1, 2)\n",
    "    k = k.view(B, S, H, D).transpose(1, 2)\n",
    "    v = v.view(B, S, H, D).transpose(1, 2)\n",
    "\n",
    "    for b in range(B):\n",
    "        assign_token(b, t, k[b], v[b])\n",
    "\n",
    "    block_mask = build_blockmask_batched(\n",
    "        page_tables={b: page_table[b] for b in range(B)},\n",
    "        seq_ids=list(range(B)),\n",
    "        num_query_tokens=S,\n",
    "        total_tokens_written=KV,\n",
    "        page_size=page_size,\n",
    "    )\n",
    "\n",
    "    out = flex_attention(q, k_cache, v_cache, block_mask=block_mask)\n",
    "    print(f\"Step {t} | out.shape: {out.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d3daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 1 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 2 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Step 3 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Evicted pid=0\n",
      "Step 4 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Evicted pid=8\n",
      "Evicted pid=1\n",
      "Step 5 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Evicted pid=9\n",
      "Evicted pid=2\n",
      "Step 6 | out.shape: torch.Size([2, 2, 1, 4])\n",
      "Evicted pid=10\n",
      "Evicted pid=3\n",
      "Step 7 | out.shape: torch.Size([2, 2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.attention.flex_attention import flex_attention, BlockMask\n",
    "\n",
    "# Config\n",
    "B, H, S, D = 2, 2, 1, 4\n",
    "page_size = 1\n",
    "max_decode_len = 8\n",
    "KV_CAPACITY = 8  # triggers eviction after 8 blocks\n",
    "device = torch.device('cuda')\n",
    "\n",
    "KV = KV_CAPACITY * page_size\n",
    "k_cache = torch.zeros((1, H, KV, D), device=device)\n",
    "v_cache = torch.zeros((1, H, KV, D), device=device)\n",
    "page_table = torch.full((B, max_decode_len), -1, dtype=torch.int32, device=device)\n",
    "sequence_table = {i: [] for i in range(B)}\n",
    "\n",
    "class LogicalBlock:\n",
    "    def __init__(self, pid):\n",
    "        self.physical_block_id = pid\n",
    "        self.token_count = 0\n",
    "        self.status = \"gpu\"\n",
    "\n",
    "class EvictionPolicy:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.active = []\n",
    "\n",
    "    def touch(self, block):\n",
    "        if block in self.active:\n",
    "            self.active.remove(block)\n",
    "        self.active.append(block)\n",
    "\n",
    "    def evict_if_needed(self):\n",
    "        if len(self.active) > self.capacity:\n",
    "            evicted = self.active.pop(0)\n",
    "            evicted.status = \"cpu\"\n",
    "            print(f\"Evicted pid={evicted.physical_block_id}\")\n",
    "            return evicted\n",
    "        return None\n",
    "\n",
    "eviction_policy = EvictionPolicy(KV_CAPACITY)\n",
    "\n",
    "\n",
    "next_pid = [0]\n",
    "def assign_token(seq_id, token_idx, k_vec, v_vec):\n",
    "    lid = token_idx // page_size\n",
    "    offset = token_idx % page_size\n",
    "    pid = seq_id * max_decode_len + token_idx\n",
    "\n",
    "    if len(sequence_table[seq_id]) <= lid:\n",
    "        block = LogicalBlock(pid)\n",
    "        eviction_policy.evict_if_needed()\n",
    "        pid = next_pid[0] % KV_CAPACITY\n",
    "        next_pid[0] += 1\n",
    "        eviction_policy.touch(block)\n",
    "        sequence_table[seq_id].append(block)\n",
    "\n",
    "    block = sequence_table[seq_id][lid]\n",
    "    flat_idx = pid * page_size + offset\n",
    "    k_cache[0, :, flat_idx, :] = k_vec.squeeze(1)\n",
    "    v_cache[0, :, flat_idx, :] = v_vec.squeeze(1)\n",
    "    page_table[seq_id, lid] = pid\n",
    "    block.token_count += 1\n",
    "    block.status = \"gpu\"\n",
    "    eviction_policy.touch(block)\n",
    "\n",
    "def build_blockmask_batched(page_tables, seq_ids, num_q, total_kv, page_size):\n",
    "    B, H = len(seq_ids), k_cache.shape[1]\n",
    "    max_blocks = total_kv // page_size\n",
    "    kv_num_blocks = torch.zeros((B, H, 1), dtype=torch.int32, device=device)\n",
    "    kv_indices = torch.full((B, H, 1, max_blocks), -1, dtype=torch.int32, device=device)\n",
    "\n",
    "    for b, seq_id in enumerate(seq_ids):\n",
    "        pids = page_tables[seq_id][page_tables[seq_id] != -1]\n",
    "        kv_num_blocks[b] = torch.full((H, 1), len(pids), dtype=torch.int32, device=device)\n",
    "        for h in range(H):\n",
    "            kv_indices[b, h, 0, :len(pids)] = pids\n",
    "\n",
    "    return BlockMask.from_kv_blocks(kv_num_blocks, kv_indices, None, None, (num_q, page_size), None, (num_q, total_kv))\n",
    "\n",
    "# Decode loop\n",
    "for t in range(max_decode_len):\n",
    "    x = torch.randn(B, S, 3 * H * D, device=device)\n",
    "    q, k, v = x.split([H * D] * 3, dim=-1)\n",
    "    q, k, v = [z.view(B, S, H, D).transpose(1, 2) for z in (q, k, v)]\n",
    "\n",
    "    for b in range(B):\n",
    "        assign_token(b, t, k[b], v[b])\n",
    "\n",
    "    block_mask = build_blockmask_batched({b: page_table[b] for b in range(B)}, list(range(B)), S, KV, page_size)\n",
    "    out = flex_attention(q, k_cache, v_cache, block_mask=block_mask)\n",
    "    print(f\"Step {t} | out.shape: {out.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import after code execution environment reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.attention.flex_attention import flex_attention, BlockMask\n",
    "\n",
    "# --- Helper functions ---\n",
    "def precompute_freqs_cis(seq_len, n_elem, base=10000, dtype=torch.float16):\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem))\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1).to(dtype=dtype, device=\"cuda\")\n",
    "\n",
    "def apply_rotary_emb(x, freqs_cis):\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    freqs_cis = freqs_cis.view(xshaped.size(0), xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack([\n",
    "        xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
    "        xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
    "    ], dim=-1)\n",
    "    return x_out2.flatten(3).type_as(x)\n",
    "\n",
    "# --- Blockmask builder ---\n",
    "def build_blockmask_batched(page_tables, seq_ids, num_q, total_kv, page_size, k_cache):\n",
    "    B, H = len(seq_ids), k_cache.shape[1]\n",
    "    # max_blocks = total_kv // page_size\n",
    "    max_blocks = max((page_tables[seq_id] != -1).sum().item() for seq_id in seq_ids)\n",
    "    max_blocks = min(max_blocks, k_cache.shape[2] // page_size)\n",
    "    device = k_cache.device\n",
    "\n",
    "    kv_num_blocks = torch.zeros((B, H, 1), dtype=torch.int32, device=device)\n",
    "    kv_indices = torch.full((B, H, 1, max_blocks), -1, dtype=torch.int32, device=device)\n",
    "    max_valid_pid = k_cache.shape[2] - 1  # e.g. 3 for shape (1, 2, 4, 4)\n",
    "    for b, seq_id in enumerate(seq_ids):\n",
    "        pids = page_tables[seq_id][page_tables[seq_id] != -1]\n",
    "        kv_num_blocks[b] = torch.full((H, 1), len(pids), dtype=torch.int32, device=device)\n",
    "        safe_pids = pids[pids <= max_valid_pid]\n",
    "        for h in range(H):\n",
    "            # kv_indices[b, h, 0, :len(pids)] = pids\n",
    "            kv_indices[b, h, 0, :safe_pids.numel()] = safe_pids\n",
    "\n",
    "\n",
    "    return BlockMask.from_kv_blocks(kv_num_blocks, kv_indices, None, None, (num_q, page_size), None, (num_q, total_kv))\n",
    "\n",
    "# --- Batched Attention Layer ---\n",
    "class SwappablePagedAttentionLayerBatched(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, head_dim, dtype, spa, max_pos=512):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.spa = spa\n",
    "        self.block_size = spa.page_size\n",
    "\n",
    "        self.wqkv = nn.Linear(hidden_dim, 3 * hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        self.wo = nn.Linear(hidden_dim, hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(seq_len=max_pos, n_elem=self.head_dim, dtype=dtype)\n",
    "\n",
    "    def forward(self, x, seq_ids, token_idxs):\n",
    "        B, S, _ = x.shape\n",
    "        kv_size = self.n_heads * self.head_dim\n",
    "        q, k, v = self.wqkv(x).split([kv_size] * 3, dim=-1)\n",
    "\n",
    "        q = q.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # for b, t in enumerate(token_idxs):\n",
    "        #     freqs = self.freqs_cis[t].unsqueeze(0)\n",
    "        #     q[b] = apply_rotary_emb(q[b].unsqueeze(0), freqs)\n",
    "        #     k[b] = apply_rotary_emb(k[b].unsqueeze(0), freqs)\n",
    "\n",
    "        q = q.clone()\n",
    "        k = k.clone()\n",
    "        for b, t in enumerate(token_idxs):\n",
    "            freqs = self.freqs_cis[t]  # (H, D//2, 2)\n",
    "            q_b = apply_rotary_emb(q[b:b+1], freqs)  # returns (1, H, S, D)\n",
    "            k_b = apply_rotary_emb(k[b:b+1], freqs)\n",
    "            q[b:b+1] = q_b\n",
    "            k[b:b+1] = k_b\n",
    "\n",
    "\n",
    "\n",
    "        for b, (sid, tidx) in enumerate(zip(seq_ids, token_idxs)):\n",
    "            self.spa.assign_token(sid, tidx, k[b], v[b])\n",
    "\n",
    "        block_mask = build_blockmask_batched(\n",
    "            page_tables=self.spa.page_table,\n",
    "            seq_ids=seq_ids,\n",
    "            num_q=S,\n",
    "            total_kv=self.spa.max_flat_idx_written,\n",
    "            page_size=self.block_size,\n",
    "            k_cache=self.spa.k_cache\n",
    "        )\n",
    "        print(\"== DEBUG INFO ==\")\n",
    "        print(\"max_flat_idx_written:\", self.spa.max_flat_idx_written)\n",
    "        print(\"q.shape:\", q.shape)\n",
    "        print(\"k_cache.shape:\", self.spa.k_cache.shape)\n",
    "        print(\"block_mask shape:\", block_mask.shape)\n",
    "        print(\"block_mask.kv_indices.max():\", block_mask.kv_indices.max().item())\n",
    "        print(\"k_cache max valid idx:\", self.spa.k_cache.shape[2] - 1)\n",
    "\n",
    "        out = flex_attention(q, self.spa.k_cache, self.spa.v_cache, block_mask=block_mask)\n",
    "        return self.wo(out.transpose(1, 2).contiguous().view(B, S, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b0a77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x, freqs_cis):\n",
    "    # x: (B, H, S, D)\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)  # (..., D/2, 2)\n",
    "    freqs_cis = freqs_cis.unsqueeze(1).unsqueeze(2)    # (1, 1, 1, D/2, 2)\n",
    "\n",
    "    # Broadcastable multiplication\n",
    "    x_out2 = torch.stack([\n",
    "        xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
    "        xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
    "    ], dim=-1)\n",
    "\n",
    "    return x_out2.flatten(3).type_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a12932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kv_indices max: 3\n",
      "kv_indices unique: tensor([2, 3], device='cuda:0', dtype=torch.int32)\n",
      "== DEBUG INFO ==\n",
      "max_flat_idx_written: 4\n",
      "q.shape: torch.Size([2, 2, 1, 4])\n",
      "k_cache.shape: torch.Size([1, 2, 4, 4])\n",
      "block_mask shape: (2, 2, 1, 4)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[32m     44\u001b[39m     x_t = torch.randn(B, \u001b[32m1\u001b[39m, hidden_dim, device=device, dtype=dtype)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     out_t = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_idxs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, 1, D_model)\u001b[39;00m\n\u001b[32m     46\u001b[39m     outputs.append(out_t)\n\u001b[32m     48\u001b[39m final_output = torch.cat(outputs, dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B, T, D_model)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mSwappablePagedAttentionLayerBatched.forward\u001b[39m\u001b[34m(self, x, seq_ids, token_idxs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mk_cache.shape:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.spa.k_cache.shape)\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mblock_mask shape:\u001b[39m\u001b[33m\"\u001b[39m, block_mask.shape)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mblock_mask.kv_indices.max():\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mblock_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkv_indices\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mk_cache max valid idx:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.spa.k_cache.shape[\u001b[32m2\u001b[39m] - \u001b[32m1\u001b[39m)\n\u001b[32m    107\u001b[39m out = flex_attention(q, \u001b[38;5;28mself\u001b[39m.spa.k_cache, \u001b[38;5;28mself\u001b[39m.spa.v_cache, block_mask=block_mask)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.attention.flex_attention import BlockMask, flex_attention\n",
    "\n",
    "# --- Config ---\n",
    "B, H, D = 2, 2, 4                   # batch, heads, head_dim\n",
    "T = 6                              # tokens per sequence\n",
    "page_size = 1\n",
    "num_blocks = 4                     # total physical cache blocks\n",
    "hidden_dim = H * D\n",
    "dtype = torch.float16\n",
    "device = \"cuda\"\n",
    "\n",
    "# --- Flat KV buffers ---\n",
    "k_cache = torch.zeros((1, H, num_blocks * page_size, D), device=device, dtype=dtype)\n",
    "v_cache = torch.zeros_like(k_cache)\n",
    "\n",
    "# --- Real PageTable and Manager ---\n",
    "pt = PageTableWithSwap(num_blocks, page_size, H, D, device=device, k_cache=k_cache, v_cache=v_cache)\n",
    "manager = KVCacheManager(page_table=pt)\n",
    "\n",
    "# --- Real SwappablePagedAttention ---\n",
    "spa = SwappablePagedAttention(\n",
    "    kv_cache_manager=manager,\n",
    "    k_cache=k_cache,\n",
    "    v_cache=v_cache,\n",
    "    page_table_tensor=torch.full((B, T), -1, dtype=torch.int32, device=device),\n",
    "    page_size=page_size,\n",
    "    layer_id=0\n",
    ")\n",
    "\n",
    "# --- The Layer ---\n",
    "layer = SwappablePagedAttentionLayerBatched(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=H,\n",
    "    head_dim=D,\n",
    "    dtype=dtype,\n",
    "    spa=spa\n",
    ")\n",
    "\n",
    "# --- Decode loop ---\n",
    "outputs = []\n",
    "for t in range(T):\n",
    "    x_t = torch.randn(B, 1, hidden_dim, device=device, dtype=dtype)\n",
    "    out_t = layer(x_t, seq_ids=list(range(B)), token_idxs=[t] * B)  # (B, 1, D_model)\n",
    "    outputs.append(out_t)\n",
    "\n",
    "final_output = torch.cat(outputs, dim=1)  # (B, T, D_model)\n",
    "\n",
    "# --- Done ---\n",
    "print(\"✅ Final output shape:\", final_output.shape)\n",
    "print(\"✅ No NaNs:\", not torch.isnan(final_output).any().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f88cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5ffad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.attention.flex_attention import flex_attention, BlockMask\n",
    "\n",
    "# === CONFIG ===\n",
    "B, H, D = 2, 2, 4                # batch size, num heads, head dim\n",
    "T = 6                           # tokens per sequence\n",
    "page_size = 1\n",
    "num_blocks = 4                  # small capacity to force evictions\n",
    "hidden_dim = H * D\n",
    "dtype = torch.float16\n",
    "device = \"cuda\"\n",
    "\n",
    "# === FLAT CACHE ===\n",
    "k_cache = torch.zeros((1, H, num_blocks * page_size, D), device=device, dtype=dtype)\n",
    "v_cache = torch.zeros_like(k_cache)\n",
    "\n",
    "# === PAGE TABLE + MANAGER ===\n",
    "pt = PageTableWithSwap(num_blocks, page_size, H, D, device=device, k_cache=k_cache, v_cache=v_cache)\n",
    "manager = KVCacheManager(page_table=pt)\n",
    "\n",
    "# === SWAPPABLE PAGER ===\n",
    "spa = SwappablePagedAttention(\n",
    "    kv_cache_manager=manager,\n",
    "    k_cache=k_cache,\n",
    "    v_cache=v_cache,\n",
    "    page_table_tensor=torch.full((B, T), -1, dtype=torch.int32, device=device),\n",
    "    page_size=page_size,\n",
    "    layer_id=0\n",
    ")\n",
    "\n",
    "# === ROTARY UTILS ===\n",
    "def precompute_freqs_cis(seq_len, n_elem, base=10000, dtype=torch.float16):\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2).float() / n_elem))\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1).to(dtype=dtype, device=\"cuda\")\n",
    "\n",
    "def apply_rotary_emb(x, freqs_cis):\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    freqs_cis = freqs_cis.unsqueeze(1).unsqueeze(2)\n",
    "    x_out2 = torch.stack([\n",
    "        xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
    "        xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
    "    ], dim=-1)\n",
    "    return x_out2.flatten(3).type_as(x)\n",
    "\n",
    "# === PATCHED BLOCKMASK BUILDER ===\n",
    "def build_blockmask_batched(page_tables, seq_ids, num_q, total_kv, page_size, k_cache):\n",
    "    B, H = len(seq_ids), k_cache.shape[1]\n",
    "    device = k_cache.device\n",
    "\n",
    "    max_blocks = k_cache.shape[2] // page_size  # 🧠 max physical blocks, not tokens\n",
    "    kv_num_blocks = torch.zeros((B, H, 1), dtype=torch.int32, device=device)\n",
    "    kv_indices = torch.full((B, H, 1, max_blocks), -1, dtype=torch.int32, device=device)\n",
    "\n",
    "    for b, sid in enumerate(seq_ids):\n",
    "        raw_pids = page_tables[sid]\n",
    "        valid_pids = raw_pids[(raw_pids >= 0) & (raw_pids < max_blocks)]\n",
    "        num_valid = valid_pids.numel()\n",
    "\n",
    "        kv_num_blocks[b] = torch.full((H, 1), num_valid, dtype=torch.int32, device=device)\n",
    "        for h in range(H):\n",
    "            kv_indices[b, h, 0, :num_valid] = valid_pids[:max_blocks]  # clamp to capacity\n",
    "            kv_indices[b, h, 0, num_valid:] = -1  # pad with -1\n",
    "\n",
    "    assert (kv_indices < max_blocks).all() | (kv_indices == -1).all(), \"🟥 kv_indices out of range\"\n",
    "\n",
    "    return BlockMask.from_kv_blocks(\n",
    "        kv_num_blocks=kv_num_blocks,\n",
    "        kv_indices=kv_indices,\n",
    "        full_kv_num_blocks=None,\n",
    "        full_kv_indices=None,\n",
    "        BLOCK_SIZE=(num_q, page_size),\n",
    "        mask_mod=None,\n",
    "        seq_lengths=(num_q, total_kv),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8320dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  2,  3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "sent\n",
      "sent\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "sent\n",
      "sent\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([0, 1, 2, 3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "sent\n",
      "sent\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([0, 1, 2, 3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "sent\n",
      "sent\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([0, 1, 2, 3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "✅ Final output shape: torch.Size([2, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# === PATCHED ATTENTION LAYER USING spa.query ===\n",
    "class SwappablePagedAttentionLayerBatched(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, head_dim, dtype, spa, max_pos=512):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.spa = spa\n",
    "        self.block_size = spa.page_size\n",
    "\n",
    "        self.wqkv = nn.Linear(hidden_dim, 3 * hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        self.wo = nn.Linear(hidden_dim, hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        self.freqs_cis = precompute_freqs_cis(max_pos, head_dim, dtype=dtype)\n",
    "\n",
    "    def forward(self, x, seq_ids, token_idxs):\n",
    "        B, S, _ = x.shape\n",
    "        qkv = self.wqkv(x)\n",
    "        q, k, v = qkv.split(self.hidden_dim, dim=-1)\n",
    "        q = q.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, S, D)\n",
    "        k = k.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q, k = q.clone(), k.clone()\n",
    "        for b, t in enumerate(token_idxs):\n",
    "            freqs = self.freqs_cis[t]\n",
    "            q[b:b+1] = apply_rotary_emb(q[b:b+1], freqs)\n",
    "            k[b:b+1] = apply_rotary_emb(k[b:b+1], freqs)\n",
    "\n",
    "        for b, (sid, tidx) in enumerate(zip(seq_ids, token_idxs)):\n",
    "            self.spa.assign_token(sid, tidx, k[b], v[b])\n",
    "\n",
    "        block_mask = build_blockmask_batched(\n",
    "            page_tables=self.spa.page_table,\n",
    "            seq_ids=seq_ids,\n",
    "            num_q=S,\n",
    "            total_kv=self.spa.max_flat_idx_written,\n",
    "            page_size=self.block_size,\n",
    "            k_cache=self.spa.k_cache\n",
    "        )\n",
    "\n",
    "        print(\"\\n🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\")\n",
    "        flat_kv_indices = block_mask.kv_indices.view(-1)\n",
    "        invalid = flat_kv_indices[(flat_kv_indices < 0) | (flat_kv_indices >= self.spa.k_cache.shape[2])]\n",
    "        print(\"🟥 Found invalid indices:\", invalid)\n",
    "        print(\"✅ Unique indices:\", torch.unique(flat_kv_indices))\n",
    "        print(\"✅ Max valid:\", self.spa.k_cache.shape[2] - 1)\n",
    "\n",
    "        out = self.spa.query(q, block_mask)  # ✅ Use SPA query for retrieval\n",
    "        return self.wo(out.transpose(1, 2).contiguous().view(B, S, -1))\n",
    "\n",
    "\n",
    "# === RUN TEST ===\n",
    "layer = SwappablePagedAttentionLayerBatched(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=H,\n",
    "    head_dim=D,\n",
    "    dtype=dtype,\n",
    "    spa=spa\n",
    ")\n",
    "\n",
    "outputs = []\n",
    "for t in range(T):\n",
    "    x_t = torch.randn(B, 1, hidden_dim, device=device, dtype=dtype)\n",
    "    out_t = layer(x_t, seq_ids=list(range(B)), token_idxs=[t] * B)\n",
    "    outputs.append(out_t)\n",
    "\n",
    "final_out = torch.cat(outputs, dim=1)\n",
    "print(\"✅ Final output shape:\", final_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c73f49a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  2,  3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "sent\n",
      "sent\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1,  0,  1,  2,  3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "sent\n",
      "sent\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([0, 1, 2, 3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "sent\n",
      "sent\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([0, 1, 2, 3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "sent\n",
      "sent\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([], device='cuda:0', dtype=torch.int32)\n",
      "✅ Unique indices: tensor([0, 1, 2, 3], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 3\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "sent\n",
      "retrieved to pid=0\n",
      "⏪ Swapping in block with pid=0\n",
      "sent\n",
      "retrieved to pid=3\n",
      "⏪ Swapping in block with pid=3\n",
      "sent\n",
      "retrieved to pid=2\n",
      "⏪ Swapping in block with pid=2\n",
      "sent\n",
      "retrieved to pid=1\n",
      "⏪ Swapping in block with pid=1\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([2, 2, 1, 4])\n",
      "  k.shape: torch.Size([1, 2, 4, 4])\n",
      "  v.shape: torch.Size([1, 2, 4, 4])\n",
      "✅ Final output shape: torch.Size([2, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.attention.flex_attention import flex_attention, BlockMask\n",
    "\n",
    "# === CONFIG ===\n",
    "B, H, D = 2, 2, 4\n",
    "T = 6\n",
    "page_size = 1\n",
    "num_blocks = 4\n",
    "hidden_dim = H * D\n",
    "dtype = torch.float16\n",
    "device = \"cuda\"\n",
    "\n",
    "# === FLAT CACHE ===\n",
    "k_cache = torch.zeros((1, H, num_blocks * page_size, D), device=device, dtype=dtype)\n",
    "v_cache = torch.zeros_like(k_cache)\n",
    "\n",
    "# === PAGE TABLE + MANAGER ===\n",
    "pt = PageTableWithSwap(num_blocks, page_size, H, D, device=device, k_cache=k_cache, v_cache=v_cache)\n",
    "manager = KVCacheManager(page_table=pt)\n",
    "\n",
    "# === SWAPPABLE PAGER ===\n",
    "spa = SwappablePagedAttention(\n",
    "    kv_cache_manager=manager,\n",
    "    k_cache=k_cache,\n",
    "    v_cache=v_cache,\n",
    "    page_table_tensor=torch.full((B, T), -1, dtype=torch.int32, device=device),\n",
    "    page_size=page_size,\n",
    "    layer_id=0\n",
    ")\n",
    "\n",
    "# === ROTARY UTILS ===\n",
    "def precompute_freqs_cis(seq_len, n_elem, base=10000, dtype=torch.float16):\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2).float() / n_elem))\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1).to(dtype=dtype, device=\"cuda\")\n",
    "\n",
    "def apply_rotary_emb(x, freqs_cis):\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    freqs_cis = freqs_cis.unsqueeze(1).unsqueeze(2)\n",
    "    x_out2 = torch.stack([\n",
    "        xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
    "        xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
    "    ], dim=-1)\n",
    "    return x_out2.flatten(3).type_as(x)\n",
    "\n",
    "# === PATCHED CAUSAL BLOCKMASK BUILDER ===\n",
    "def build_blockmask_batched(page_tables, seq_ids, token_idxs, num_q, total_kv, page_size, k_cache):\n",
    "    B, H = len(seq_ids), k_cache.shape[1]\n",
    "    device = k_cache.device\n",
    "\n",
    "    max_blocks = k_cache.shape[2] // page_size\n",
    "    kv_num_blocks = torch.zeros((B, H, 1), dtype=torch.int32, device=device)\n",
    "    kv_indices = torch.full((B, H, 1, max_blocks), -1, dtype=torch.int32, device=device)\n",
    "\n",
    "    for b, sid in enumerate(seq_ids):\n",
    "        raw_pids = page_tables[sid]\n",
    "        max_token_idx = token_idxs[b]\n",
    "        max_block_idx = max_token_idx // page_size\n",
    "\n",
    "        causal_pids = raw_pids[:max_block_idx + 1]\n",
    "        valid_pids = causal_pids[(causal_pids >= 0) & (causal_pids < max_blocks)]\n",
    "        num_valid = valid_pids.numel()\n",
    "\n",
    "        kv_num_blocks[b] = torch.full((H, 1), num_valid, dtype=torch.int32, device=device)\n",
    "        for h in range(H):\n",
    "            kv_indices[b, h, 0, :num_valid] = valid_pids[:max_blocks]\n",
    "            kv_indices[b, h, 0, num_valid:] = -1\n",
    "\n",
    "    assert (kv_indices < max_blocks).all() | (kv_indices == -1).all(), \"🟥 kv_indices out of range\"\n",
    "\n",
    "    return BlockMask.from_kv_blocks(\n",
    "        kv_num_blocks=kv_num_blocks,\n",
    "        kv_indices=kv_indices,\n",
    "        full_kv_num_blocks=None,\n",
    "        full_kv_indices=None,\n",
    "        BLOCK_SIZE=(num_q, page_size),\n",
    "        mask_mod=None,\n",
    "        seq_lengths=(num_q, total_kv),\n",
    "    )\n",
    "\n",
    "# === ATTENTION LAYER ===\n",
    "class SwappablePagedAttentionLayerBatched(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, head_dim, dtype, spa, max_pos=512):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.spa = spa\n",
    "        self.block_size = spa.page_size\n",
    "\n",
    "        self.wqkv = nn.Linear(hidden_dim, 3 * hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        self.wo = nn.Linear(hidden_dim, hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        self.freqs_cis = precompute_freqs_cis(max_pos, head_dim, dtype=dtype)\n",
    "\n",
    "    def forward(self, x, seq_ids, token_idxs):\n",
    "        B, S, _ = x.shape\n",
    "        qkv = self.wqkv(x)\n",
    "        q, k, v = qkv.split(self.hidden_dim, dim=-1)\n",
    "        q = q.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q, k = q.clone(), k.clone()\n",
    "        for b, t in enumerate(token_idxs):\n",
    "            freqs = self.freqs_cis[t]\n",
    "            q[b:b+1] = apply_rotary_emb(q[b:b+1], freqs)\n",
    "            k[b:b+1] = apply_rotary_emb(k[b:b+1], freqs)\n",
    "\n",
    "        for b, (sid, tidx) in enumerate(zip(seq_ids, token_idxs)):\n",
    "            self.spa.assign_token(sid, tidx, k[b], v[b])\n",
    "\n",
    "        block_mask = build_blockmask_batched(\n",
    "            page_tables=self.spa.page_table,\n",
    "            seq_ids=seq_ids,\n",
    "            token_idxs=token_idxs,\n",
    "            num_q=S,\n",
    "            total_kv=self.spa.max_flat_idx_written,\n",
    "            page_size=self.block_size,\n",
    "            k_cache=self.spa.k_cache\n",
    "        )\n",
    "\n",
    "        print(\"\\n🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\")\n",
    "        flat_kv_indices = block_mask.kv_indices.view(-1)\n",
    "        invalid = flat_kv_indices[(flat_kv_indices < 0) | (flat_kv_indices >= self.spa.k_cache.shape[2])]\n",
    "        print(\"🟥 Found invalid indices:\", invalid)\n",
    "        print(\"✅ Unique indices:\", torch.unique(flat_kv_indices))\n",
    "        print(\"✅ Max valid:\", self.spa.k_cache.shape[2] - 1)\n",
    "\n",
    "        out = self.spa.query(q, block_mask)\n",
    "        return self.wo(out.transpose(1, 2).contiguous().view(B, S, -1))\n",
    "\n",
    "# === RUN TEST ===\n",
    "layer = SwappablePagedAttentionLayerBatched(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=H,\n",
    "    head_dim=D,\n",
    "    dtype=dtype,\n",
    "    spa=spa\n",
    ")\n",
    "\n",
    "outputs = []\n",
    "for t in range(T):\n",
    "    x_t = torch.randn(B, 1, hidden_dim, device=device, dtype=dtype)\n",
    "    out_t = layer(x_t, seq_ids=list(range(B)), token_idxs=[t] * B)\n",
    "    outputs.append(out_t)\n",
    "\n",
    "final_out = torch.cat(outputs, dim=1)\n",
    "print(\"✅ Final output shape:\", final_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80974209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention.flex_attention import BlockMask\n",
    "class SwappablePagedAttention:\n",
    "    def __init__(self, kv_cache_manager, k_cache, v_cache, page_table_tensor, page_size, layer_id):\n",
    "        self.kv_cache_manager = kv_cache_manager\n",
    "        self.k_cache = k_cache\n",
    "        self.v_cache = v_cache\n",
    "        self.page_table = page_table_tensor\n",
    "        self.page_size = page_size\n",
    "        self.layer_id = layer_id\n",
    "        self.max_flat_idx_written = 0\n",
    "\n",
    "    def assign_token(self, seq_id, token_idx, key_vec, value_vec):\n",
    "        logical_block_id = token_idx // self.page_size\n",
    "        offset = token_idx % self.page_size\n",
    "\n",
    "        # ⬇️ Expand page_table if the logical block id exceeds current columns\n",
    "        if logical_block_id >= self.page_table.shape[1]:\n",
    "            pad = logical_block_id - self.page_table.shape[1] + 1\n",
    "            new_table = torch.full(\n",
    "                (self.page_table.shape[0], self.page_table.shape[1] + pad),\n",
    "                fill_value=-1,\n",
    "                dtype=self.page_table.dtype,\n",
    "                device=self.page_table.device\n",
    "            )\n",
    "            new_table[:, :self.page_table.shape[1]] = self.page_table\n",
    "            self.page_table = new_table\n",
    "\n",
    "        # Get or grow the block list for this sequence/layer\n",
    "        block_list = self.kv_cache_manager.sequence_table[seq_id][self.layer_id]\n",
    "        if len(block_list) <= logical_block_id:\n",
    "            for _ in range(logical_block_id - len(block_list) + 1):\n",
    "                new_block = LogicalBlock()\n",
    "                pid = self.kv_cache_manager.page_table.allocate_block(new_block)\n",
    "                self.kv_cache_manager.page_table.eviction_policy.register(new_block)\n",
    "                block_list.append(new_block)\n",
    "\n",
    "        block = block_list[logical_block_id]\n",
    "\n",
    "        # If it's not on GPU, swap it in\n",
    "        if block.status == \"cpu\":\n",
    "            self.kv_cache_manager.page_table.swap_to_gpu(block)\n",
    "\n",
    "        pid = block.physical_block_id\n",
    "        # Sanity check:\n",
    "        if block.physical_block_id is None:\n",
    "            raise RuntimeError(f\"Token {token_idx}: block still has no physical_block_id after swap.\")\n",
    "\n",
    "        flat_idx = pid * self.page_size + offset\n",
    "\n",
    "        # key_vec, value_vec: [H, S, D]\n",
    "        # token_idx: scalar index into S\n",
    "\n",
    "        assert key_vec.ndim == 3, f\"Expected key_vec to be [H, S, D], got {key_vec.shape}\"\n",
    "        assert value_vec.ndim == 3, f\"Expected value_vec to be [H, S, D], got {value_vec.shape}\"\n",
    "        assert 0 <= token_idx < key_vec.shape[1], f\"token_idx {token_idx} out of bounds for seq_len={key_vec.shape[1]}\"\n",
    "\n",
    "        self.k_cache[0, :, flat_idx, :] = key_vec[:, token_idx, :]  # [H, D]\n",
    "        self.v_cache[0, :, flat_idx, :] = value_vec[:, token_idx, :]\n",
    "\n",
    "\n",
    "        block.token_count += 1\n",
    "        if seq_id >= self.page_table.shape[0] or logical_block_id >= self.page_table.shape[1]:\n",
    "            print(f\"🚨 Resize needed: seq_id={seq_id}, logical_block_id={logical_block_id}, current shape={self.page_table.shape}\")\n",
    "\n",
    "        self.page_table[seq_id, logical_block_id] = pid\n",
    "\n",
    "        self.max_flat_idx_written = max(self.max_flat_idx_written, flat_idx + 1)\n",
    "\n",
    "        print(f\"[assign_token] token_idx={token_idx} | flat_idx={flat_idx}\")\n",
    "        print(f\"[assign_token] key_vec.shape: {key_vec.shape}\")\n",
    "        print(f\"[assign_token] storing slice: {key_vec[:, token_idx, :].shape}\")\n",
    "\n",
    "\n",
    "    def build_blockmask(self, num_query_tokens, total_tokens_written):\n",
    "        from torch.nn.attention.flex_attention import BlockMask\n",
    "\n",
    "        assert total_tokens_written <= self.k_cache.shape[2], \"KV length exceeds cache capacity\"\n",
    "        B, H = 1, self.k_cache.shape[1]\n",
    "        device = self.k_cache.device\n",
    "\n",
    "        logical_to_physical = self.page_table[0]  # single seq_id only\n",
    "        valid = logical_to_physical != -1\n",
    "        pids = logical_to_physical[valid]\n",
    "        total_blocks = pids.numel()\n",
    "\n",
    "        kv_indices = torch.full((B, H, 1, total_blocks), -1, dtype=torch.int32, device=device)\n",
    "        kv_num_blocks = torch.full((B, H, 1), total_blocks, dtype=torch.int32, device=device)\n",
    "\n",
    "        for h in range(H):\n",
    "            for i in range(total_blocks):\n",
    "                kv_indices[0, h, 0, i] = pids[i].item()\n",
    "        print(\">> max_flat_idx_written:\", self.max_flat_idx_written)\n",
    "        print(\">> kv_num_blocks.shape:\", kv_num_blocks.shape)\n",
    "        print(\">> kv_num_blocks:\", kv_num_blocks)\n",
    "        print(\">> kv_indices.shape:\", kv_indices.shape)\n",
    "        print(\">> kv_indices:\", kv_indices)\n",
    "\n",
    "        block_mask = BlockMask.from_kv_blocks(\n",
    "            kv_num_blocks=kv_num_blocks,\n",
    "            kv_indices=kv_indices,\n",
    "            full_kv_num_blocks=None,\n",
    "            full_kv_indices=None,\n",
    "            BLOCK_SIZE=(num_query_tokens, self.page_size),\n",
    "            mask_mod=None,\n",
    "            seq_lengths=(num_query_tokens, total_tokens_written)  # <- exact KV length\n",
    "        )\n",
    "\n",
    "        # Crop block mask to match token counts\n",
    "        return block_mask._adjust(num_query_tokens, total_tokens_written)\n",
    "\n",
    "    def query(self, q, block_mask):\n",
    "        from torch.nn.attention.flex_attention import flex_attention\n",
    "\n",
    "        # 🔁 Ensure required blocks are on GPU\n",
    "        for block_list in self.kv_cache_manager.sequence_table.values():\n",
    "            for block in block_list[self.layer_id]:\n",
    "                if block.status == \"cpu\":\n",
    "                    self.kv_cache_manager.page_table.swap_to_gpu(block)\n",
    "                    print(f\"⏪ Swapping in block with pid={block.physical_block_id}\")\n",
    "\n",
    "        print(\"Calling flex_attention with:\")\n",
    "        print(\"  q.shape:\", q.shape)\n",
    "        print(\"  k.shape:\", self.k_cache.shape)\n",
    "        print(\"  v.shape:\", self.v_cache.shape)\n",
    "\n",
    "\n",
    "        return flex_attention(\n",
    "            q, self.k_cache, self.v_cache,\n",
    "            block_mask=block_mask,\n",
    "            score_mod=None\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2584464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ATTENTION LAYER TO DROP-IN ===\n",
    "class SwappablePagedAttentionLayerBatched(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, head_dim, dtype, spa, max_pos=512):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.spa = spa\n",
    "        self.block_size = spa.page_size\n",
    "\n",
    "        self.wqkv = nn.Linear(hidden_dim, 3 * hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        self.wo = nn.Linear(hidden_dim, hidden_dim, bias=False, device=\"cuda\", dtype=dtype)\n",
    "        # Inside __init__ of SwappablePagedAttentionLayerBatched\n",
    "        self.freqs_cis = precompute_freqs_cis(max_pos, head_dim, dtype=dtype).to(\"cuda\")\n",
    "\n",
    "\n",
    "    def forward(self, x, seq_ids, token_idxs):\n",
    "        B, S, _ = x.shape\n",
    "        x = x.to(dtype=self.wqkv.weight.dtype)\n",
    "        qkv = self.wqkv(x)\n",
    "        q, k, v = qkv.split(self.hidden_dim, dim=-1)\n",
    "        q = q.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q, k = q.clone(), k.clone()\n",
    "        # for b, t in enumerate(token_idxs):\n",
    "        #     freqs = self.freqs_cis[t]\n",
    "        #     q[b:b+1] = apply_rotary_emb(q[b:b+1], freqs)\n",
    "        #     k[b:b+1] = apply_rotary_emb(k[b:b+1], freqs)\n",
    "        for b, t in enumerate(token_idxs):\n",
    "            if t >= self.freqs_cis.shape[0]:\n",
    "                raise ValueError(f\"Token index {t} exceeds precomputed freqs_cis range\")\n",
    "            freqs = self.freqs_cis[t]\n",
    "            q[b:b+1] = apply_rotary_emb(q[b:b+1], freqs)\n",
    "            k[b:b+1] = apply_rotary_emb(k[b:b+1], freqs)\n",
    "\n",
    "\n",
    "        for b, (sid, tidx) in enumerate(zip(seq_ids, token_idxs)):\n",
    "            self.spa.assign_token(sid, tidx, k[b], v[b])\n",
    "\n",
    "        block_mask = build_blockmask_batched(\n",
    "            page_tables=self.spa.page_table,\n",
    "            seq_ids=seq_ids,\n",
    "            token_idxs=token_idxs,\n",
    "            num_q=S,\n",
    "            total_kv=self.spa.max_flat_idx_written,\n",
    "            page_size=self.block_size,\n",
    "            k_cache=self.spa.k_cache\n",
    "        )\n",
    "\n",
    "        print(\"\\n🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\")\n",
    "        flat_kv_indices = block_mask.kv_indices.view(-1)\n",
    "        invalid = flat_kv_indices[(flat_kv_indices < 0) | (flat_kv_indices >= self.spa.k_cache.shape[2])]\n",
    "        print(\"🟥 Found invalid indices:\", invalid)\n",
    "        print(\"✅ Unique indices:\", torch.unique(flat_kv_indices))\n",
    "        print(\"✅ Max valid:\", self.spa.k_cache.shape[2] - 1)\n",
    "\n",
    "        out = self.spa.query(q, block_mask)\n",
    "        return self.wo(out.transpose(1, 2).contiguous().view(B, S, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28759460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x, freqs_cis_t):\n",
    "    B, H, S, D = x.shape\n",
    "    assert D % 2 == 0, \"Head dim must be even\"\n",
    "\n",
    "    x = x.float().reshape(B, H, S, D // 2, 2)\n",
    "    freqs_cis_t = freqs_cis_t.unsqueeze(0).unsqueeze(0).unsqueeze(0)  # (1,1,1,D//2,2)\n",
    "\n",
    "    re = x[..., 0] * freqs_cis_t[..., 0] - x[..., 1] * freqs_cis_t[..., 1]\n",
    "    im = x[..., 1] * freqs_cis_t[..., 0] + x[..., 0] * freqs_cis_t[..., 1]\n",
    "\n",
    "    return torch.stack((re, im), dim=-1).flatten(3).type_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f303e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[assign_token] token_idx=0 | flat_idx=31\n",
      "[assign_token] key_vec.shape: torch.Size([12, 3, 64])\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "\n",
      "🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\n",
      "🟥 Found invalid indices: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "✅ Unique indices: tensor([-1, 31], device='cuda:0', dtype=torch.int32)\n",
      "✅ Max valid: 31\n",
      "Calling flex_attention with:\n",
      "  q.shape: torch.Size([1, 12, 3, 64])\n",
      "  k.shape: torch.Size([1, 12, 32, 64])\n",
      "  v.shape: torch.Size([1, 12, 32, 64])\n",
      "✅ Output shape: torch.Size([1, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch import nn\n",
    "from torch.nn.attention.flex_attention import BlockMask\n",
    "\n",
    "# === CONFIG ===\n",
    "B, H, D = 1, 12, 64  # GPT-2 small: 12 heads, 64 dim each\n",
    "page_size = 1\n",
    "num_blocks = 32\n",
    "hidden_dim = H * D\n",
    "dtype = torch.float16\n",
    "device = \"cuda\"\n",
    "\n",
    "# === FLAT KV CACHE ===\n",
    "k_cache = torch.zeros((1, H, num_blocks * page_size, D), device=device, dtype=dtype)\n",
    "v_cache = torch.zeros_like(k_cache)\n",
    "\n",
    "# === PAGE TABLE + MANAGER ===\n",
    "pt = PageTableWithSwap(num_blocks, page_size, H, D, device=device, k_cache=k_cache, v_cache=v_cache)\n",
    "manager = KVCacheManager(page_table=pt)\n",
    "\n",
    "# === SPA INSTANCE ===\n",
    "spa = SwappablePagedAttention(\n",
    "    kv_cache_manager=manager,\n",
    "    k_cache=k_cache,\n",
    "    v_cache=v_cache,\n",
    "    page_table_tensor=torch.full((1, 1024), -1, dtype=torch.int32, device=device),  # GPT-2 max length\n",
    "    page_size=page_size,\n",
    "    layer_id=0\n",
    ")\n",
    "\n",
    "# === PATCHED LAYER ===\n",
    "patched_layer = SwappablePagedAttentionLayerBatched(\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=H,\n",
    "    head_dim=D,\n",
    "    dtype=dtype,\n",
    "    spa=spa\n",
    ")\n",
    "\n",
    "class GPT2CompatibleAttention(nn.Module):\n",
    "    def __init__(self, swappable_layer, layer_id):\n",
    "        super().__init__()\n",
    "        self.attn_core = swappable_layer\n",
    "        self.layer_id = layer_id\n",
    "        self.token_idx = 0  # Tracks where we are in decoding\n",
    "\n",
    "    def forward(self, hidden_states, layer_past=None, attention_mask=None, **kwargs):\n",
    "        B, S, D = hidden_states.shape\n",
    "        seq_ids = list(range(B))\n",
    "        token_idxs = [self.token_idx] * B\n",
    "        self.token_idx += S\n",
    "\n",
    "        return (self.attn_core(hidden_states, seq_ids, token_idxs), None)\n",
    "\n",
    "\n",
    "# === LOAD GPT-2 ===\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").eval().cuda()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model.config.use_cache = False  # disable past_key_values\n",
    "\n",
    "# === PATCH FIRST LAYER ===\n",
    "model.transformer.h[0].attn = GPT2CompatibleAttention(patched_layer, layer_id=0)\n",
    "\n",
    "# === TEST INFERENCE ===\n",
    "prompt = \"The sky is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids).logits\n",
    "    print(\"✅ Output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13f3770f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SwappablePagedAttention.__init__() got an unexpected keyword argument 'num_seqs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m head_dim = \u001b[32m64\u001b[39m\n\u001b[32m      6\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m spa = \u001b[43mSwappablePagedAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_seqs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# ✅ supports `seq_id` in [0, batch_size - 1]\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: SwappablePagedAttention.__init__() got an unexpected keyword argument 'num_seqs'"
     ]
    }
   ],
   "source": [
    "batch_size = 2  # or whatever batch size you're using\n",
    "num_blocks = 128\n",
    "page_size = 1  # assuming 1 token per block\n",
    "num_heads = 12\n",
    "head_dim = 64\n",
    "device = \"cuda\"\n",
    "\n",
    "spa = SwappablePagedAttention(\n",
    "    num_seqs=batch_size,         # ✅ supports `seq_id` in [0, batch_size - 1]\n",
    "    num_blocks=num_blocks,\n",
    "    page_size=page_size,\n",
    "    num_heads=num_heads,\n",
    "    head_dim=head_dim,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7175747",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "max_new_tokens = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Set pad_token to eos_token (common workaround for GPT2)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize input prompt (same prompt duplicated for simplicity)\n",
    "input_ids = tokenizer([prompt] * batch_size, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "# Create attention mask if needed\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "# Decode state\n",
    "generated = input_ids.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8179a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[assign_token] token_idx=3 | flat_idx=28\n",
      "[assign_token] key_vec.shape: torch.Size([12, 4, 64])\n",
      "[assign_token] storing slice: torch.Size([12, 64])\n",
      "🚨 Resize needed: seq_id=1, logical_block_id=3, current shape=torch.Size([1, 1024])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:922\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    910\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    911\u001b[39m         block.\u001b[34m__call__\u001b[39m,\n\u001b[32m    912\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    919\u001b[39m         output_attentions,\n\u001b[32m    920\u001b[39m     )\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:404\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[39m\n\u001b[32m    402\u001b[39m residual = hidden_states\n\u001b[32m    403\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m attn_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m attn_output = attn_outputs[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[32m    413\u001b[39m outputs = attn_outputs[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mGPT2CompatibleAttention.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m token_idxs = [\u001b[38;5;28mself\u001b[39m.token_idx] * B\n\u001b[32m     52\u001b[39m \u001b[38;5;28mself\u001b[39m.token_idx += S\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_idxs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishaa\\Documents\\HOMEWORK\\EFF AI LABS\\PROJECT\\effai_proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mSwappablePagedAttentionLayerBatched.forward\u001b[39m\u001b[34m(self, x, seq_ids, token_idxs)\u001b[39m\n\u001b[32m     36\u001b[39m     k[b:b+\u001b[32m1\u001b[39m] = apply_rotary_emb(k[b:b+\u001b[32m1\u001b[39m], freqs)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b, (sid, tidx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(seq_ids, token_idxs)):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspa\u001b[49m\u001b[43m.\u001b[49m\u001b[43massign_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m block_mask = build_blockmask_batched(\n\u001b[32m     43\u001b[39m     page_tables=\u001b[38;5;28mself\u001b[39m.spa.page_table,\n\u001b[32m     44\u001b[39m     seq_ids=seq_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     k_cache=\u001b[38;5;28mself\u001b[39m.spa.k_cache\n\u001b[32m     50\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔍 DEBUG BLOCK_MASK BEFORE FLEX_ATTENTION\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mSwappablePagedAttention.assign_token\u001b[39m\u001b[34m(self, seq_id, token_idx, key_vec, value_vec)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m seq_id >= \u001b[38;5;28mself\u001b[39m.page_table.shape[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m logical_block_id >= \u001b[38;5;28mself\u001b[39m.page_table.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🚨 Resize needed: seq_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, logical_block_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogical_block_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, current shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.page_table.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpage_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseq_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogical_block_id\u001b[49m\u001b[43m]\u001b[49m = pid\n\u001b[32m     67\u001b[39m \u001b[38;5;28mself\u001b[39m.max_flat_idx_written = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_flat_idx_written, flat_idx + \u001b[32m1\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[assign_token] token_idx=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | flat_idx=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflat_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model(input_ids=input_ids, attention_mask=attention_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effai_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
